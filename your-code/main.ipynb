{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:10.254059Z",
     "start_time": "2021-05-10T15:22:04.022337Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1 - Explore the Scikit-Learn Datasets\n",
    "\n",
    "Before starting to work on our own datasets, let's first explore the datasets that are included in this Python library. These datasets have been cleaned and formatted for use in ML algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load the diabetes dataset. Do this in the cell below by importing the datasets and then loading the dataset  to the `diabetes` variable using the `load_diabetes()` function ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:10.456703Z",
     "start_time": "2021-05-10T15:22:10.307381Z"
    }
   },
   "outputs": [],
   "source": [
    "diabetes = load_diabetes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore this variable by looking at the different attributes (keys) of `diabetes`. Note that the `load_diabetes` function does not return dataframes. It returns you a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:10.564350Z",
     "start_time": "2021-05-10T15:22:10.523075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'DESCR', 'feature_names', 'data_filename', 'target_filename'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next step is to read the description of the dataset. \n",
    "\n",
    "Print the description in the cell below using the `DESCR` attribute of the `diabetes` variable. Read the data description carefully to fully understand what each column represents.\n",
    "\n",
    "*Hint: If your output is ill-formatted by displaying linebreaks as `\\n`, it means you are not using the `print` function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:10.591494Z",
     "start_time": "2021-05-10T15:22:10.574500Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _diabetes_dataset:\n",
      "\n",
      "Diabetes dataset\n",
      "----------------\n",
      "\n",
      "Ten baseline variables, age, sex, body mass index, average blood\n",
      "pressure, and six blood serum measurements were obtained for each of n =\n",
      "442 diabetes patients, as well as the response of interest, a\n",
      "quantitative measure of disease progression one year after baseline.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "  :Number of Instances: 442\n",
      "\n",
      "  :Number of Attributes: First 10 columns are numeric predictive values\n",
      "\n",
      "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
      "\n",
      "  :Attribute Information:\n",
      "      - age     age in years\n",
      "      - sex\n",
      "      - bmi     body mass index\n",
      "      - bp      average blood pressure\n",
      "      - s1      tc, T-Cells (a type of white blood cells)\n",
      "      - s2      ldl, low-density lipoproteins\n",
      "      - s3      hdl, high-density lipoproteins\n",
      "      - s4      tch, thyroid stimulating hormone\n",
      "      - s5      ltg, lamotrigine\n",
      "      - s6      glu, blood sugar level\n",
      "\n",
      "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n",
      "\n",
      "Source URL:\n",
      "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
      "\n",
      "For more information see:\n",
      "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
      "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n"
     ]
    }
   ],
   "source": [
    "print(diabetes.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the data description, answer the following questions:\n",
    "\n",
    "1. How many attributes are there in the data? What do they mean?\n",
    "\n",
    "1. What is the relation between `diabetes['data']` and `diabetes['target']`?\n",
    "\n",
    "1. How many records are there in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:10.624310Z",
     "start_time": "2021-05-10T15:22:10.613980Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. 10 attributes, two of them are demographics (age and sex), other two are general medical\n",
    "# measurements (bmi and blood pressure) and the other six are related to blood serums\n",
    "\n",
    "# 2. diabetes['data'] includes the described 10 columns and diabetes['target'] is an extra column that\n",
    "# is what we want to predict with ML\n",
    "\n",
    "# 3. 442 instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now explore what are contained in the *data* portion as well as the *target* portion of `diabetes`. \n",
    "\n",
    "Scikit-learn typically takes in 2D numpy arrays as input (though pandas dataframes are also accepted). Inspect the shape of `data` and `target`. Confirm they are consistent with the data description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:10.676495Z",
     "start_time": "2021-05-10T15:22:10.653826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = diabetes['data']\n",
    "target = diabetes['target']\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:10.719674Z",
     "start_time": "2021-05-10T15:22:10.686658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Perform Supervised Learning on the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data have already been split to predictor (*data*) and response (*target*) variables. Given this information, we'll apply what we have previously learned about linear regression and apply the algorithm to the diabetes dataset.\n",
    "\n",
    "#### Let's briefly revisit the linear regression formula:\n",
    "\n",
    "\n",
    "$y = \\beta_0 + \\beta_1·X_1 + \\beta_2·X_2 + ... + \\beta_n·X_n + \\epsilon$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "- $X_1-X_n$: data \n",
    "- $\\beta_0$: intercept \n",
    "- $\\beta_1-\\beta_n$: coefficients \n",
    "- $\\epsilon$: error (cannot explained by model)\n",
    "- $y$: target\n",
    "\n",
    "Also take a look at the `sklearn.linear_model.LinearRegression` [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "\n",
    "#### In the cell below, import the `linear_model` class from `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:11.779898Z",
     "start_time": "2021-05-10T15:22:10.771252Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new instance of the linear regression model and assign the new instance to the variable `diabetes_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:11.824797Z",
     "start_time": "2021-05-10T15:22:11.813547Z"
    }
   },
   "outputs": [],
   "source": [
    "diabetes_model = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, let's split the training and test data.\n",
    "\n",
    "Define `diabetes_data_train`, `diabetes_target_train`, `diabetes_data_test`, and `diabetes_target_test`. Use the last 20 records for the test data and the rest for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:11.888653Z",
     "start_time": "2021-05-10T15:22:11.834146Z"
    }
   },
   "outputs": [],
   "source": [
    "diabetes_data_train = data[:-20]\n",
    "diabetes_target_train = target[:-20]\n",
    "\n",
    "diabetes_data_test = data[-20:]\n",
    "diabetes_target_test = target[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the training data and target to `diabetes_model`. Print the *intercept* and *coefficients* of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:12.001713Z",
     "start_time": "2021-05-10T15:22:11.915381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept:  152.76430691633442\n",
      "Coefficients:  [ 3.03499549e-01 -2.37639315e+02  5.10530605e+02  3.27736980e+02\n",
      " -8.14131709e+02  4.92814588e+02  1.02848452e+02  1.84606489e+02\n",
      "  7.43519617e+02  7.60951722e+01]\n"
     ]
    }
   ],
   "source": [
    "diabetes_model.fit(diabetes_data_train,diabetes_target_train)\n",
    "\n",
    "print('Intercept: ',diabetes_model.intercept_)\n",
    "print('Coefficients: ',diabetes_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the results\n",
    "\n",
    "From the outputs you should have seen:\n",
    "\n",
    "- The intercept is a float number.\n",
    "- The coefficients are an array containing 10 float numbers.\n",
    "\n",
    "This is the linear regression model fitted to your training dataset.\n",
    "\n",
    "#### Using your fitted linear regression model, predict the *y* of `diabetes_data_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:12.019333Z",
     "start_time": "2021-05-10T15:22:12.005264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[198. 155. 173. 112. 165. 131. 259. 100. 117. 124. 218.  61. 132. 120.\n",
      "  53. 194. 103. 124. 211.  53.]\n"
     ]
    }
   ],
   "source": [
    "diabetes_pred_test = diabetes_model.predict(diabetes_data_test)\n",
    "print(np.round(diabetes_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print your `diabetes_target_test` and compare with the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:12.047928Z",
     "start_time": "2021-05-10T15:22:12.027193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[233.  91. 111. 152. 120.  67. 310.  94. 183.  66. 173.  72.  49.  64.\n",
      "  48. 178. 104. 132. 220.  57.]\n",
      "[ 35. -64. -62.  40. -45. -64.  51.  -6.  66. -58. -45.  11. -83. -56.\n",
      "  -5. -16.   1.   8.   9.   4.]\n"
     ]
    }
   ],
   "source": [
    "print(diabetes_target_test)\n",
    "\n",
    "print(diabetes_target_test-np.round(diabetes_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is `diabetes_target_test` exactly the same as the model prediction? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:12.088235Z",
     "start_time": "2021-05-10T15:22:12.061229Z"
    }
   },
   "outputs": [],
   "source": [
    "# No, it's not the same (and in some cases it's quite bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 1 - Hypothesis Testing with `statsmodels`\n",
    "\n",
    "After generating the linear regression model from the dataset, you probably wonder: then what? What is the statistical way to know if my model is reliable or not?\n",
    "\n",
    "Good question. We'll discuss that using Scikit-Learn in Challenge 5. But for now, let's use a fool-proof way by using the ([Linear Regression class of StatsModels](https://www.statsmodels.org/dev/regression.html)) which can also conduct linear regression analysis plus much more such as calcuating the F-score of the linear model as well as the standard errors and t-scores for each coefficient. The F-score and t-scores will tell you whether you can trust your linear model.\n",
    "\n",
    "To understand the statistical meaning of conducting hypothesis testing (e.g. F-test, t-test) for slopes, read [this webpage](https://online.stat.psu.edu/stat501/lesson/6/6.4) at your leisure time. We'll give you a brief overview next.\n",
    "\n",
    "* The F-test of your linear model is to verify whether at least one of your coefficients is significantly different from zero. Translating that into the *null hypothesis* and *alternative hypothesis*, that is:\n",
    "\n",
    "    $H_0 : \\beta_1 = \\beta_2 = ... = \\beta_{10} = 0$\n",
    "\n",
    "    $H_a$ : At least one $\\beta_j \\ne 0$ (for j = 1, 2, ..., 10)\n",
    "    \n",
    "    \n",
    "* The t-tests on each coefficient is to check whether the confidence interval for the variable contains zero. If the confidence interval contains zero, it means the null hypothesis for that variable is not rejected. In other words, this particular vaiable is not contributing to your linear model and you can remove it from your formula.\n",
    "\n",
    "Read the documentations of [StatsModels Linear Regression](https://www.statsmodels.org/dev/regression.html) as well as its [`OLS` class](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html) which stands for *ordinary least squares*.\n",
    "\n",
    "#### In the next cell, analyze `diabetes_data_train` and `diabetes_target_train` with the linear regression model of `statsmodels`. Print the fit summary.\n",
    "\n",
    "Your output should look like:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"../images/statsmodels.png\" width=600/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:14.559420Z",
     "start_time": "2021-05-10T15:22:12.094385Z"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.regression.linear_model import OLS\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:14.665378Z",
     "start_time": "2021-05-10T15:22:14.564774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.512\n",
      "Model:                            OLS   Adj. R-squared:                  0.500\n",
      "Method:                 Least Squares   F-statistic:                     43.16\n",
      "Date:                Mon, 10 May 2021   Prob (F-statistic):           4.64e-58\n",
      "Time:                        17:22:14   Log-Likelihood:                -2281.1\n",
      "No. Observations:                 422   AIC:                             4584.\n",
      "Df Residuals:                     411   BIC:                             4629.\n",
      "Df Model:                          10                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        152.7643      2.658     57.469      0.000     147.539     157.990\n",
      "x1             0.3035     61.286      0.005      0.996    -120.169     120.776\n",
      "x2          -237.6393     62.837     -3.782      0.000    -361.162    -114.117\n",
      "x3           510.5306     68.156      7.491      0.000     376.553     644.508\n",
      "x4           327.7370     66.876      4.901      0.000     196.275     459.199\n",
      "x5          -814.1317    424.044     -1.920      0.056   -1647.697      19.434\n",
      "x6           492.8146    344.227      1.432      0.153    -183.850    1169.480\n",
      "x7           102.8485    219.463      0.469      0.640    -328.561     534.258\n",
      "x8           184.6065    167.336      1.103      0.271    -144.334     513.547\n",
      "x9           743.5196    175.359      4.240      0.000     398.807    1088.232\n",
      "x10           76.0952     68.293      1.114      0.266     -58.152     210.343\n",
      "==============================================================================\n",
      "Omnibus:                        1.544   Durbin-Watson:                   2.026\n",
      "Prob(Omnibus):                  0.462   Jarque-Bera (JB):                1.421\n",
      "Skew:                           0.004   Prob(JB):                        0.491\n",
      "Kurtosis:                       2.716   Cond. No.                         224.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "diabetes_data_train_sm = sm.add_constant(diabetes_data_train)\n",
    "model_ols = sm.OLS(diabetes_target_train,diabetes_data_train_sm)\n",
    "res_ols = model_ols.fit()\n",
    "\n",
    "print(res_ols.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting hypothesis testing results\n",
    "\n",
    "Answer the following questions in the cell below:\n",
    "\n",
    "1. What is the F-score of your linear model and is the null hypothesis rejected?\n",
    "\n",
    "1. Does any of the t-tests of the coefficients produce a confidence interval containing zero? What are they?\n",
    "\n",
    "1. How will you modify your linear reguression model according to the test results above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:14.755663Z",
     "start_time": "2021-05-10T15:22:14.713916Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. The F-score is 43.16 so we can reject the null hypothesis\n",
    "\n",
    "# 2. Yes, a few of them. Age, T-cells, low-density lipoproteins, high-density lipoproteins,\n",
    "# thyroid stimulating hormone,blood sugar level\n",
    "\n",
    "# 3. We can drop data from those columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Peform Supervised Learning on a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have dealt with data that has been formatted for scikit-learn, let's look at data that we will need to format ourselves.\n",
    "\n",
    "In the next cell, load the `auto-mpg.csv` file included in this folder and assign it to a variable called `auto`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:14.777898Z",
     "start_time": "2021-05-10T15:22:14.760774Z"
    }
   },
   "outputs": [],
   "source": [
    "auto = pd.read_csv('../data/auto-mpg.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the first 5 rows using the `head()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:14.852109Z",
     "start_time": "2021-05-10T15:22:14.782745Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horse_power</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "      <th>car_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>\\t\"chevrolet chevelle malibu\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>\\t\"buick skylark 320\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>\\t\"plymouth satellite\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>\\t\"amc rebel sst\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>\\t\"ford torino\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2790</td>\n",
       "      <td>15.6</td>\n",
       "      <td>82</td>\n",
       "      <td>\\t\"ford mustang gl\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>44.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2130</td>\n",
       "      <td>24.6</td>\n",
       "      <td>82</td>\n",
       "      <td>\\t\"vw pickup\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>32.0</td>\n",
       "      <td>4</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2295</td>\n",
       "      <td>11.6</td>\n",
       "      <td>82</td>\n",
       "      <td>\\t\"dodge rampage\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>28.0</td>\n",
       "      <td>4</td>\n",
       "      <td>120.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2625</td>\n",
       "      <td>18.6</td>\n",
       "      <td>82</td>\n",
       "      <td>\\t\"ford ranger\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>31.0</td>\n",
       "      <td>4</td>\n",
       "      <td>119.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2720</td>\n",
       "      <td>19.4</td>\n",
       "      <td>82</td>\n",
       "      <td>\\t\"chevy s-10\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mpg  cylinders  displacement  horse_power  weight  acceleration  \\\n",
       "0    18.0          8         307.0        130.0    3504          12.0   \n",
       "1    15.0          8         350.0        165.0    3693          11.5   \n",
       "2    18.0          8         318.0        150.0    3436          11.0   \n",
       "3    16.0          8         304.0        150.0    3433          12.0   \n",
       "4    17.0          8         302.0        140.0    3449          10.5   \n",
       "..    ...        ...           ...          ...     ...           ...   \n",
       "393  27.0          4         140.0         86.0    2790          15.6   \n",
       "394  44.0          4          97.0         52.0    2130          24.6   \n",
       "395  32.0          4         135.0         84.0    2295          11.6   \n",
       "396  28.0          4         120.0         79.0    2625          18.6   \n",
       "397  31.0          4         119.0         82.0    2720          19.4   \n",
       "\n",
       "     model_year                       car_name  \n",
       "0            70  \\t\"chevrolet chevelle malibu\"  \n",
       "1            70          \\t\"buick skylark 320\"  \n",
       "2            70         \\t\"plymouth satellite\"  \n",
       "3            70              \\t\"amc rebel sst\"  \n",
       "4            70                \\t\"ford torino\"  \n",
       "..          ...                            ...  \n",
       "393          82            \\t\"ford mustang gl\"  \n",
       "394          82                  \\t\"vw pickup\"  \n",
       "395          82              \\t\"dodge rampage\"  \n",
       "396          82                \\t\"ford ranger\"  \n",
       "397          82                 \\t\"chevy s-10\"  \n",
       "\n",
       "[398 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the data to ensure that all numeric columns are correctly detected as such by pandas. If a column is misclassified as object, coerce it to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:14.916910Z",
     "start_time": "2021-05-10T15:22:14.878289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 398 entries, 0 to 397\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   mpg           398 non-null    float64\n",
      " 1   cylinders     398 non-null    int64  \n",
      " 2   displacement  398 non-null    float64\n",
      " 3   horse_power   392 non-null    float64\n",
      " 4   weight        398 non-null    int64  \n",
      " 5   acceleration  398 non-null    float64\n",
      " 6   model_year    398 non-null    int64  \n",
      " 7   car_name      398 non-null    object \n",
      "dtypes: float64(4), int64(3), object(1)\n",
      "memory usage: 25.0+ KB\n"
     ]
    }
   ],
   "source": [
    "auto.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the newest model year and the oldest model year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:15.039506Z",
     "start_time": "2021-05-10T15:22:14.941244Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horse_power</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "      <th>car_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>\\t\"chevrolet chevelle malibu\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>4732</td>\n",
       "      <td>18.5</td>\n",
       "      <td>70</td>\n",
       "      <td>\\t\"hi 1200d\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>4382</td>\n",
       "      <td>13.5</td>\n",
       "      <td>70</td>\n",
       "      <td>\\t\"dodge d200\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4376</td>\n",
       "      <td>15.0</td>\n",
       "      <td>70</td>\n",
       "      <td>\\t\"chevy c20\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.0</td>\n",
       "      <td>8</td>\n",
       "      <td>360.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>4615</td>\n",
       "      <td>14.0</td>\n",
       "      <td>70</td>\n",
       "      <td>\\t\"ford f250\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>112.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2640</td>\n",
       "      <td>18.6</td>\n",
       "      <td>82</td>\n",
       "      <td>\\t\"chevrolet cavalier wagon\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>28.0</td>\n",
       "      <td>4</td>\n",
       "      <td>112.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2605</td>\n",
       "      <td>19.6</td>\n",
       "      <td>82</td>\n",
       "      <td>\\t\"chevrolet cavalier\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>28.0</td>\n",
       "      <td>4</td>\n",
       "      <td>120.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2625</td>\n",
       "      <td>18.6</td>\n",
       "      <td>82</td>\n",
       "      <td>\\t\"ford ranger\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>36.0</td>\n",
       "      <td>4</td>\n",
       "      <td>107.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2205</td>\n",
       "      <td>14.5</td>\n",
       "      <td>82</td>\n",
       "      <td>\\t\"honda accord\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>31.0</td>\n",
       "      <td>4</td>\n",
       "      <td>119.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2720</td>\n",
       "      <td>19.4</td>\n",
       "      <td>82</td>\n",
       "      <td>\\t\"chevy s-10\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mpg  cylinders  displacement  horse_power  weight  acceleration  \\\n",
       "0    18.0          8         307.0        130.0    3504          12.0   \n",
       "28    9.0          8         304.0        193.0    4732          18.5   \n",
       "27   11.0          8         318.0        210.0    4382          13.5   \n",
       "26   10.0          8         307.0        200.0    4376          15.0   \n",
       "25   10.0          8         360.0        215.0    4615          14.0   \n",
       "..    ...        ...           ...          ...     ...           ...   \n",
       "368  27.0          4         112.0         88.0    2640          18.6   \n",
       "367  28.0          4         112.0         88.0    2605          19.6   \n",
       "396  28.0          4         120.0         79.0    2625          18.6   \n",
       "381  36.0          4         107.0         75.0    2205          14.5   \n",
       "397  31.0          4         119.0         82.0    2720          19.4   \n",
       "\n",
       "     model_year                       car_name  \n",
       "0            70  \\t\"chevrolet chevelle malibu\"  \n",
       "28           70                   \\t\"hi 1200d\"  \n",
       "27           70                 \\t\"dodge d200\"  \n",
       "26           70                  \\t\"chevy c20\"  \n",
       "25           70                  \\t\"ford f250\"  \n",
       "..          ...                            ...  \n",
       "368          82   \\t\"chevrolet cavalier wagon\"  \n",
       "367          82         \\t\"chevrolet cavalier\"  \n",
       "396          82                \\t\"ford ranger\"  \n",
       "381          82               \\t\"honda accord\"  \n",
       "397          82                 \\t\"chevy s-10\"  \n",
       "\n",
       "[398 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto.sort_values('model_year')\n",
    "\n",
    "#Newest: 82\n",
    "#Oldest: 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dataset for missing values and remove all rows containing at least one missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:15.157506Z",
     "start_time": "2021-05-10T15:22:15.042615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horse_power</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "      <th>car_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>\\t\"chevrolet chevelle malibu\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>\\t\"buick skylark 320\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>\\t\"plymouth satellite\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>\\t\"amc rebel sst\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>\\t\"ford torino\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2790</td>\n",
       "      <td>15.6</td>\n",
       "      <td>82</td>\n",
       "      <td>\\t\"ford mustang gl\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>44.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2130</td>\n",
       "      <td>24.6</td>\n",
       "      <td>82</td>\n",
       "      <td>\\t\"vw pickup\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>32.0</td>\n",
       "      <td>4</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2295</td>\n",
       "      <td>11.6</td>\n",
       "      <td>82</td>\n",
       "      <td>\\t\"dodge rampage\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>28.0</td>\n",
       "      <td>4</td>\n",
       "      <td>120.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2625</td>\n",
       "      <td>18.6</td>\n",
       "      <td>82</td>\n",
       "      <td>\\t\"ford ranger\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>31.0</td>\n",
       "      <td>4</td>\n",
       "      <td>119.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2720</td>\n",
       "      <td>19.4</td>\n",
       "      <td>82</td>\n",
       "      <td>\\t\"chevy s-10\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>392 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mpg  cylinders  displacement  horse_power  weight  acceleration  \\\n",
       "0    18.0          8         307.0        130.0    3504          12.0   \n",
       "1    15.0          8         350.0        165.0    3693          11.5   \n",
       "2    18.0          8         318.0        150.0    3436          11.0   \n",
       "3    16.0          8         304.0        150.0    3433          12.0   \n",
       "4    17.0          8         302.0        140.0    3449          10.5   \n",
       "..    ...        ...           ...          ...     ...           ...   \n",
       "387  27.0          4         140.0         86.0    2790          15.6   \n",
       "388  44.0          4          97.0         52.0    2130          24.6   \n",
       "389  32.0          4         135.0         84.0    2295          11.6   \n",
       "390  28.0          4         120.0         79.0    2625          18.6   \n",
       "391  31.0          4         119.0         82.0    2720          19.4   \n",
       "\n",
       "     model_year                       car_name  \n",
       "0            70  \\t\"chevrolet chevelle malibu\"  \n",
       "1            70          \\t\"buick skylark 320\"  \n",
       "2            70         \\t\"plymouth satellite\"  \n",
       "3            70              \\t\"amc rebel sst\"  \n",
       "4            70                \\t\"ford torino\"  \n",
       "..          ...                            ...  \n",
       "387          82            \\t\"ford mustang gl\"  \n",
       "388          82                  \\t\"vw pickup\"  \n",
       "389          82              \\t\"dodge rampage\"  \n",
       "390          82                \\t\"ford ranger\"  \n",
       "391          82                 \\t\"chevy s-10\"  \n",
       "\n",
       "[392 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto = auto.dropna().reset_index(drop=True)\n",
    "auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the frequency table for the `cylinders` column using the `value_counts()` function. How many possible values of cylinders are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:15.406937Z",
     "start_time": "2021-05-10T15:22:15.333405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    199\n",
       "8    103\n",
       "6     83\n",
       "3      4\n",
       "5      3\n",
       "Name: cylinders, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto['cylinders'].value_counts()\n",
    "\n",
    "# 3, 4, 5, 6 or 8 cylinders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to generate a linear regression model that will predict mpg. To do this, first drop the `car_name` column since it does not contain any quantitative data. Next separate the dataframe to predictor and response variables. Separate those into test and training data with 80% of the data in the training set and the remainder in the test set. \n",
    "\n",
    "Assign the predictor and response training data to `X_train` and `y_train` respectively. Similarly, assign the predictor and response test data to `X_test` and `y_test`.\n",
    "\n",
    "*Hint: To separate data for training and test, use the `train_test_split` method we used in previous labs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:15.632320Z",
     "start_time": "2021-05-10T15:22:15.459468Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "auto.drop(columns='car_name',errors='ignore',inplace=True)\n",
    "\n",
    "y = auto['mpg']\n",
    "X = auto.drop(columns='mpg')\n",
    "\n",
    "X_train, X_test, y_train, y_test = tts(X,y,test_size=0.2,random_state=1123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will processed and peform linear regression on this data to predict the mpg for each vehicle. \n",
    "\n",
    "#### In the next cell, create an instance of the linear regression model and call it `auto_model`. Fit `auto_model` with your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:15.753102Z",
     "start_time": "2021-05-10T15:22:15.657227Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "auto_model = LinearRegression()\n",
    "\n",
    "auto_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Challenge 4 - Evaluate the Model\n",
    "\n",
    "In addition to evaluating your model with F-test and t-test, you can also use the *Coefficient of Determination* (a.k.a. *r squared score*). This method does not simply tell *yes* or *no* about the model fit but instead indicates how much variation can be explained by the model. Based on the r squared score, you can decide whether to improve your model in order to obtain a better fit.\n",
    "\n",
    "You can learn about the r squared score [here](https://en.wikipedia.org/wiki/Coefficient_of_determination). Its formula is:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"../images/r-squared.png\" width=300/></div>\n",
    "\n",
    "where:\n",
    "\n",
    "* yi is an actual data point.\n",
    "* ŷi is the corresponding data point on the estimated regression line.\n",
    "\n",
    "By adding the squares of the difference between all yi-ŷi pairs, we have a measure called SSE (*error sum of squares*) which is an application of the r squared score to indicate the extent to which the estimated regression model is different from the actual data. And we attribute that difference to the random error that is unavoidable in the real world. Obviously, we want the SSE value to be as small as possible.\n",
    "\n",
    "#### In the next cell, compute the predicted *y* based on `X_train` and call it `y_pred`. Then calcualte the r squared score between `y_pred` and `y_train` which indicates how well the estimated regression model fits the training data.\n",
    "\n",
    "*Hint: r squared score can be calculated using `sklearn.metrics.r2_score` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:15.858286Z",
     "start_time": "2021-05-10T15:22:15.786117Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8090803904769007"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred = auto_model.predict(X_train)\n",
    "\n",
    "r2 = r2_score(y_train,y_pred)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our next step is to evaluate the model using the test data. \n",
    "\n",
    "We would like to ensure that our model is not overfitting the data. This means that our model was made to fit too closely to the training data by being overly complex. If a model is overfitted, it is not generalizable to data outside the training data. In that case, we need to reduce the complexity of the model by removing certain features (variables).\n",
    "\n",
    "In the cell below, use the model to generate the predicted values for the test data and assign them to `y_test_pred`. Compute the r squared score of the predicted `y_test_pred` and the oberserved `y_test` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:15.924999Z",
     "start_time": "2021-05-10T15:22:15.867243Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7984289217568107"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = auto_model.predict(X_test)\n",
    "\n",
    "r2_test = r2_score(y_test,y_test_pred)\n",
    "r2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explaining the results\n",
    "\n",
    "Please, compare the results for the training and test sets and comment. What can you say?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:15.959032Z",
     "start_time": "2021-05-10T15:22:15.948013Z"
    }
   },
   "outputs": [],
   "source": [
    "# The results are not very good as the R^2 value is quite low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 5 - Improve the Model Fit\n",
    "\n",
    "While the most common way to improve the fit of a model is by using [regularization](https://datanice.github.io/machine-learning-101-what-is-regularization-interactive.html), there are other simpler ways to improve model fit. The first is to create a simpler model. The second is to increase the train sample size.\n",
    "\n",
    "Let us start with the easier option and increase our train sample size to 90% of the data. Create a new test train split and name the new predictors and response variables `X_train09`, `X_test09`, `y_train09`, `y_test09`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:16.013375Z",
     "start_time": "2021-05-10T15:22:15.980867Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train09, X_test09, y_train09, y_test09 = tts(X,y,test_size=0.1,random_state=1123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a new linear regression model. Name this model `auto_model09`. Fit the model to the new sample (training) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:16.077170Z",
     "start_time": "2021-05-10T15:22:16.027309Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_model09 = LinearRegression()\n",
    "\n",
    "auto_model09.fit(X_train09,y_train09)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the predicted values and r squared score for our new model and new sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:16.233541Z",
     "start_time": "2021-05-10T15:22:16.123021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8122008277362573"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred09 = auto_model09.predict(X_train09)\n",
    "\n",
    "r209 = r2_score(y_train09,y_pred09)\n",
    "r209"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the r squared score for the smaller test set. Is there an improvement in the test r squared?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:16.326482Z",
     "start_time": "2021-05-10T15:22:16.272063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7821162403466526"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred09 = auto_model09.predict(X_test09)\n",
    "\n",
    "r209_test = r2_score(y_test09,y_test_pred09)\n",
    "r209_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 2 - Backward Elimination \n",
    "\n",
    "The main way to produce a simpler linear regression model is to reduce the number of variables used in the model. In scikit-learn, we can do this by using recursive feature elimination. You can read more about RFE [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html).\n",
    "\n",
    "In the next cell, we will import RFE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:16.767934Z",
     "start_time": "2021-05-10T15:22:16.463866Z"
    }
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the documentation and initialize an RFE model using the `auto_model` linear regression model. Set `n_features_to_select=3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:06:46.113931Z",
     "start_time": "2021-05-10T16:06:46.078954Z"
    }
   },
   "outputs": [],
   "source": [
    "selector = RFE(auto_model, n_features_to_select=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model and print the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:06:48.018260Z",
     "start_time": "2021-05-10T16:06:47.862344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 4, 1, 1, 1])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.fit(X_train, y_train)\n",
    "selector.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:22:17.011325Z",
     "start_time": "2021-05-10T15:22:16.951168Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horse_power</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cylinders  displacement  horse_power  weight  acceleration  model_year\n",
       "0          8         307.0        130.0    3504          12.0          70\n",
       "1          8         350.0        165.0    3693          11.5          70\n",
       "2          8         318.0        150.0    3436          11.0          70\n",
       "3          8         304.0        150.0    3433          12.0          70\n",
       "4          8         302.0        140.0    3449          10.5          70"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head() #Keep weight, acceleration and model_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance is ranked from most important (1) to least important (4). Generate a model with the three most important features. The features correspond to variable names. For example, feature 1 is `cylinders` and feature 2 is `displacement`.\n",
    "\n",
    "Perform a test-train split on this reduced column data and call the split data `X_train_reduced`, `X_test_reduced`, `y_test_reduced`, `y_train_reduced`. Use an 80% split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:06:51.512860Z",
     "start_time": "2021-05-10T16:06:51.471969Z"
    }
   },
   "outputs": [],
   "source": [
    "X_reduced = X.loc[:,selector.ranking_==1]\n",
    "\n",
    "X_train_reduced, X_test_reduced, y_train_reduced, y_test_reduced = tts(X_reduced,y,test_size=0.2,random_state=1123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a new model called `auto_model_reduced` and fit this model. Then proceed to compute the r squared score for the model. Did this cause an improvement in the r squared score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:06:54.758459Z",
     "start_time": "2021-05-10T16:06:54.644339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8083630160171372"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_model_reduced = LinearRegression()\n",
    "\n",
    "auto_model_reduced.fit(X_train_reduced,y_train_reduced)\n",
    "\n",
    "y_pred_train_reduced = auto_model_reduced.predict(X_train_reduced)\n",
    "\n",
    "r2_reduced = r2_score(y_train_reduced,y_pred_train_reduced)\n",
    "r2_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T16:06:56.799645Z",
     "start_time": "2021-05-10T16:06:56.736490Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8014353768238786"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test_reduced = auto_model_reduced.predict(X_test_reduced)\n",
    "\n",
    "r2_reduced_test = r2_score(y_test_reduced,y_pred_test_reduced)\n",
    "r2_reduced_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "You may obtain the impression from this lab that without knowing statistical methods in depth, it is difficult to make major progress in machine learning. That is correct. If you are motivated to become a data scientist, statistics is the subject you must be proficient in and there is no shortcut. \n",
    "\n",
    "Completing these labs is not likely to make you a data scientist. But you will have a good sense about what are there in machine learning and what are good for you. In your future career, you can choose one of the three tracks:\n",
    "\n",
    "* Data scientists who need to be proficient in statistical methods.\n",
    "\n",
    "* Data engineers who need to be good at programming.\n",
    "\n",
    "* Data integration specialists who are business or content experts but also understand data and programming. This cross-disciplinary track brings together data, technology, and business and will be in high demands in the next decade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thursday Lab - Tree based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:25:24.174192Z",
     "start_time": "2021-05-10T15:25:24.140010Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = tts(X,y,test_size=0.2,random_state=1123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:53:02.427146Z",
     "start_time": "2021-05-10T15:53:02.306693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.900848261568489\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor as dtr\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "model = dtr(max_depth=4)\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "print(r2_score(y_train,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:53:04.533518Z",
     "start_time": "2021-05-10T15:53:04.470284Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.867054940325094"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_t = model.predict(X_test)\n",
    "\n",
    "r2_score(y_test,y_pred_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:34:21.725822Z",
     "start_time": "2021-05-10T15:34:21.673589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:50:23.909616Z",
     "start_time": "2021-05-10T15:49:43.359276Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=DecisionTreeRegressor(),\n",
       "             param_grid={'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
       "                                       13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
       "                                       23, 24, 25, 26, 27, 28, 29, 30, ...]})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = dtr()\n",
    "\n",
    "tree = GridSearchCV(mod,param_grid={'max_depth':list(range(1,400))})\n",
    "\n",
    "tree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:50:25.864332Z",
     "start_time": "2021-05-10T15:50:25.403640Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01284132, 0.01247888, 0.004878  , 0.01489782, 0.01996155,\n",
       "        0.04390922, 0.01004319, 0.00872598, 0.02082   , 0.01179876,\n",
       "        0.01000762, 0.00866957, 0.00620551, 0.00577335, 0.01440463,\n",
       "        0.01788268, 0.00996904, 0.03438773, 0.05433621, 0.00512743,\n",
       "        0.0077498 , 0.0057826 , 0.01184325, 0.03607335, 0.00730362,\n",
       "        0.00845866, 0.00875163, 0.01967115, 0.01320848, 0.00904255,\n",
       "        0.00842338, 0.00945396, 0.00416884, 0.00605183, 0.00660377,\n",
       "        0.00659246, 0.00631013, 0.00498948, 0.00511708, 0.0053349 ,\n",
       "        0.00518775, 0.01838078, 0.00912814, 0.00923724, 0.00693617,\n",
       "        0.0093905 , 0.01829925, 0.00955434, 0.00686331, 0.00609131,\n",
       "        0.00517054, 0.00535278, 0.00554199, 0.00527201, 0.00490112,\n",
       "        0.00421662, 0.00802364, 0.00826206, 0.01059499, 0.00681024,\n",
       "        0.00673089, 0.00606523, 0.00759296, 0.00445538, 0.00427108,\n",
       "        0.00409575, 0.00520225, 0.00454254, 0.00460172, 0.00764942,\n",
       "        0.00400109, 0.00535336, 0.00749402, 0.00491343, 0.00634594,\n",
       "        0.007126  , 0.00522132, 0.00502839, 0.00568218, 0.00437999,\n",
       "        0.00477419, 0.00544238, 0.00519819, 0.00585122, 0.00471182,\n",
       "        0.00472174, 0.00556173, 0.0046155 , 0.00852318, 0.00563941,\n",
       "        0.00449591, 0.00594473, 0.00563841, 0.00881701, 0.00557117,\n",
       "        0.00583839, 0.00563784, 0.00514126, 0.00606699, 0.00435123,\n",
       "        0.00390143, 0.0065567 , 0.00448346, 0.00817885, 0.01836858,\n",
       "        0.00807624, 0.00434198, 0.00523219, 0.01992841, 0.00472245,\n",
       "        0.00680733, 0.00592375, 0.01240478, 0.01273212, 0.00418258,\n",
       "        0.00751462, 0.00497489, 0.01142874, 0.02286477, 0.00430236,\n",
       "        0.0042872 , 0.00445418, 0.00751271, 0.03422208, 0.00550799,\n",
       "        0.0072329 , 0.02186127, 0.00596743, 0.00512915, 0.05689874,\n",
       "        0.01699305, 0.05637522, 0.12551379, 0.0067018 , 0.00680385,\n",
       "        0.01332502, 0.01996822, 0.01214371, 0.01562715, 0.02025852,\n",
       "        0.00986991, 0.01139617, 0.01338038, 0.04524922, 0.01504073,\n",
       "        0.01279106, 0.00845509, 0.0077322 , 0.00844078, 0.02751279,\n",
       "        0.0221005 , 0.02478218, 0.03009682, 0.00661674, 0.00875931,\n",
       "        0.02225842, 0.0109724 , 0.00977798, 0.01364026, 0.0048048 ,\n",
       "        0.01143064, 0.00708141, 0.00874991, 0.00954456, 0.00985804,\n",
       "        0.01012859, 0.02233844, 0.03326898, 0.00621452, 0.00896988,\n",
       "        0.00709605, 0.00708842, 0.00584788, 0.00538421, 0.01050205,\n",
       "        0.03575912, 0.015311  , 0.03150597, 0.03559637, 0.01286259,\n",
       "        0.01089787, 0.00759864, 0.00888944, 0.00837803, 0.00994425,\n",
       "        0.00927649, 0.0119401 , 0.01518168, 0.00506864, 0.00954413,\n",
       "        0.01113086, 0.01471815, 0.01018772, 0.00928378, 0.01296387,\n",
       "        0.04014792, 0.02032599, 0.01903181, 0.02509704, 0.01011748,\n",
       "        0.03495393, 0.00903139, 0.0110086 , 0.03438568, 0.01009541,\n",
       "        0.00851922, 0.01147628, 0.01134748, 0.01948862, 0.0214294 ,\n",
       "        0.03051887, 0.01872263, 0.0097362 , 0.04746122, 0.01624303,\n",
       "        0.01007953, 0.00977859, 0.00915627, 0.00745387, 0.0120749 ,\n",
       "        0.00740752, 0.00585904, 0.01371598, 0.02239404, 0.00855708,\n",
       "        0.0184206 , 0.01724892, 0.03552508, 0.01668777, 0.03585467,\n",
       "        0.02566814, 0.00502143, 0.00735779, 0.00710306, 0.04403954,\n",
       "        0.01324935, 0.00606899, 0.00516472, 0.01153359, 0.01869807,\n",
       "        0.01066966, 0.04132142, 0.03912435, 0.00821867, 0.00873227,\n",
       "        0.00457988, 0.01312442, 0.03999739, 0.00601811, 0.00698061,\n",
       "        0.00944743, 0.01372795, 0.01409383, 0.037112  , 0.02548676,\n",
       "        0.02407846, 0.01455059, 0.02039089, 0.00547404, 0.00638833,\n",
       "        0.00507474, 0.00655532, 0.00721521, 0.00778809, 0.00657325,\n",
       "        0.00476561, 0.00502987, 0.01403966, 0.01584716, 0.00674491,\n",
       "        0.00443873, 0.00522285, 0.00460944, 0.00850735, 0.00396652,\n",
       "        0.00859547, 0.0068902 , 0.00750866, 0.00876021, 0.00438266,\n",
       "        0.00439982, 0.00549989, 0.00531001, 0.00633268, 0.00632176,\n",
       "        0.00534053, 0.00497122, 0.0052412 , 0.00538254, 0.00528674,\n",
       "        0.0056797 , 0.00506206, 0.00756764, 0.00548587, 0.00605769,\n",
       "        0.00496082, 0.00456181, 0.00593162, 0.00432281, 0.00461478,\n",
       "        0.00521555, 0.00415549, 0.00643625, 0.00578241, 0.00557361,\n",
       "        0.00571456, 0.00518389, 0.00585184, 0.00600758, 0.00456948,\n",
       "        0.00746102, 0.01378946, 0.02574134, 0.00380244, 0.00867682,\n",
       "        0.00524387, 0.00614009, 0.00792923, 0.00512486, 0.00524802,\n",
       "        0.00422416, 0.00488653, 0.00516953, 0.0069912 , 0.00652013,\n",
       "        0.00633221, 0.00486188, 0.00620198, 0.00408325, 0.00521541,\n",
       "        0.00379639, 0.00979257, 0.00428624, 0.00509944, 0.00422373,\n",
       "        0.00705667, 0.0080935 , 0.00623922, 0.00627003, 0.00804043,\n",
       "        0.00466385, 0.00527439, 0.00511642, 0.0044476 , 0.00508766,\n",
       "        0.00577831, 0.00612521, 0.00665064, 0.00472937, 0.00567541,\n",
       "        0.01073418, 0.00454364, 0.00600839, 0.00424013, 0.00593739,\n",
       "        0.00463881, 0.00789866, 0.00829163, 0.00434732, 0.00456772,\n",
       "        0.00448961, 0.00380492, 0.00577478, 0.00625196, 0.00459681,\n",
       "        0.00686226, 0.00596471, 0.01267262, 0.00585742, 0.00740275,\n",
       "        0.00780926, 0.0065238 , 0.00518594, 0.00396318, 0.00455766,\n",
       "        0.0045733 , 0.00640717, 0.01095948, 0.00517402, 0.00470619,\n",
       "        0.005966  , 0.00656028, 0.00497575, 0.00586882, 0.03136873,\n",
       "        0.00971818, 0.00481749, 0.00694017, 0.00932269, 0.02383585,\n",
       "        0.01136742, 0.01456904, 0.00884485, 0.00860023, 0.0106215 ,\n",
       "        0.00506458, 0.00713906, 0.00861077, 0.0106607 ]),\n",
       " 'std_fit_time': array([0.00732278, 0.00825553, 0.0016601 , 0.01758923, 0.00923183,\n",
       "        0.05931916, 0.00702879, 0.00417738, 0.00858146, 0.00453496,\n",
       "        0.00507128, 0.0057632 , 0.00339119, 0.00177355, 0.00604746,\n",
       "        0.02133799, 0.0031199 , 0.03553854, 0.02859402, 0.0004079 ,\n",
       "        0.00425614, 0.00305171, 0.00648028, 0.03657999, 0.00167079,\n",
       "        0.00648908, 0.00224262, 0.01786779, 0.00765917, 0.00465578,\n",
       "        0.00222492, 0.00952896, 0.0012557 , 0.0028672 , 0.00286197,\n",
       "        0.00192985, 0.00373467, 0.00275301, 0.00189554, 0.00258051,\n",
       "        0.00223439, 0.00768674, 0.00794685, 0.00462164, 0.0019638 ,\n",
       "        0.01034977, 0.01812479, 0.00767761, 0.0025887 , 0.00284765,\n",
       "        0.00169468, 0.00216691, 0.00093572, 0.00168541, 0.00104584,\n",
       "        0.00086326, 0.00632447, 0.00227594, 0.00407628, 0.00273031,\n",
       "        0.00348651, 0.00350823, 0.0036336 , 0.00142361, 0.00137315,\n",
       "        0.00085487, 0.00135482, 0.00192504, 0.00124452, 0.00492061,\n",
       "        0.00096454, 0.00312501, 0.00435015, 0.00141079, 0.00300112,\n",
       "        0.00268163, 0.00199924, 0.00156725, 0.00223574, 0.00078593,\n",
       "        0.00141688, 0.00296942, 0.00203087, 0.00292716, 0.00134995,\n",
       "        0.00069727, 0.00168708, 0.00161907, 0.00337248, 0.00383249,\n",
       "        0.00155277, 0.00229369, 0.00252067, 0.00638738, 0.00297901,\n",
       "        0.00157863, 0.00318911, 0.00179654, 0.00290142, 0.00064982,\n",
       "        0.00063413, 0.00481585, 0.00173389, 0.00646997, 0.01260845,\n",
       "        0.00343962, 0.00073856, 0.00164595, 0.01276801, 0.00117491,\n",
       "        0.0034497 , 0.0028544 , 0.009268  , 0.0097143 , 0.00130083,\n",
       "        0.00513519, 0.00150918, 0.01187105, 0.01728943, 0.00097196,\n",
       "        0.0013664 , 0.00169687, 0.00272004, 0.05274545, 0.00169781,\n",
       "        0.00479444, 0.02050227, 0.00315055, 0.00262366, 0.07176813,\n",
       "        0.00507113, 0.09127233, 0.21347029, 0.00373042, 0.00231485,\n",
       "        0.00641025, 0.01408632, 0.00698986, 0.00873539, 0.01029326,\n",
       "        0.00652636, 0.00511025, 0.00499377, 0.04998508, 0.01331172,\n",
       "        0.01177746, 0.00281152, 0.00489749, 0.00312553, 0.02551497,\n",
       "        0.00650341, 0.02498959, 0.01801142, 0.0025858 , 0.00268615,\n",
       "        0.01682555, 0.00944099, 0.00601375, 0.00752159, 0.00154133,\n",
       "        0.00922466, 0.00495118, 0.00847425, 0.00716476, 0.00531655,\n",
       "        0.00434656, 0.02155977, 0.02957524, 0.00285135, 0.00204671,\n",
       "        0.00324551, 0.00405832, 0.00245655, 0.00154851, 0.00602291,\n",
       "        0.02755109, 0.0073074 , 0.03008602, 0.02102584, 0.00776848,\n",
       "        0.00554043, 0.00425114, 0.00366322, 0.00468106, 0.00456839,\n",
       "        0.00487045, 0.00817703, 0.00745058, 0.00211178, 0.00671956,\n",
       "        0.00732037, 0.00633289, 0.01115812, 0.00949897, 0.01051949,\n",
       "        0.0365815 , 0.01077469, 0.01224245, 0.02350596, 0.00564761,\n",
       "        0.03703551, 0.00488156, 0.0076338 , 0.03390509, 0.00293321,\n",
       "        0.00827298, 0.00554645, 0.01029714, 0.00893556, 0.01971347,\n",
       "        0.00868655, 0.0099832 , 0.00342948, 0.03215538, 0.01090359,\n",
       "        0.00718884, 0.00588476, 0.00565261, 0.00490665, 0.01117874,\n",
       "        0.00355251, 0.00078872, 0.00941729, 0.01255541, 0.00713573,\n",
       "        0.0071414 , 0.00812894, 0.04549433, 0.00669145, 0.03347046,\n",
       "        0.0173441 , 0.00127646, 0.00768026, 0.00369407, 0.03132061,\n",
       "        0.01049682, 0.00306832, 0.00218347, 0.00438504, 0.01926568,\n",
       "        0.00813941, 0.0549655 , 0.03677386, 0.0052321 , 0.00374465,\n",
       "        0.00140047, 0.00554134, 0.04452775, 0.00294758, 0.00119978,\n",
       "        0.0081062 , 0.00773183, 0.01300258, 0.03415386, 0.00894335,\n",
       "        0.01010819, 0.00952931, 0.02224067, 0.00135655, 0.00255985,\n",
       "        0.00164986, 0.00308801, 0.00583651, 0.00445025, 0.0021935 ,\n",
       "        0.00190772, 0.00156542, 0.00555486, 0.0099408 , 0.00411461,\n",
       "        0.00065723, 0.00319743, 0.00077161, 0.00684911, 0.0008721 ,\n",
       "        0.00302391, 0.00382539, 0.00417228, 0.00510615, 0.00144123,\n",
       "        0.0009224 , 0.00237195, 0.00151878, 0.00338882, 0.00499238,\n",
       "        0.00104953, 0.00196536, 0.00189358, 0.00376975, 0.00240924,\n",
       "        0.00186462, 0.00266166, 0.00322437, 0.00181463, 0.00402496,\n",
       "        0.00181487, 0.00124421, 0.00206088, 0.00203578, 0.00108857,\n",
       "        0.00142158, 0.00059854, 0.00237679, 0.00227101, 0.00150128,\n",
       "        0.00231919, 0.00163381, 0.00247266, 0.0032832 , 0.00063857,\n",
       "        0.0056809 , 0.00992187, 0.01925039, 0.00027236, 0.0081785 ,\n",
       "        0.00031698, 0.00417047, 0.00437904, 0.00207782, 0.00242542,\n",
       "        0.00081826, 0.00150902, 0.00111992, 0.00118811, 0.00200667,\n",
       "        0.0029578 , 0.00132711, 0.0037722 , 0.00108081, 0.00189845,\n",
       "        0.00075497, 0.00575953, 0.00088497, 0.00061472, 0.00081417,\n",
       "        0.00490678, 0.00455864, 0.00322993, 0.00307475, 0.00173599,\n",
       "        0.00093567, 0.00120744, 0.00195038, 0.00163856, 0.0013774 ,\n",
       "        0.00252433, 0.00204724, 0.00153588, 0.0014638 , 0.00218643,\n",
       "        0.00956573, 0.00197027, 0.00344197, 0.00104267, 0.00354967,\n",
       "        0.00038044, 0.00599812, 0.00132669, 0.00073545, 0.00082099,\n",
       "        0.00089204, 0.00090954, 0.00238586, 0.00285475, 0.00179832,\n",
       "        0.00264727, 0.00316746, 0.00423877, 0.00307837, 0.00366416,\n",
       "        0.00649624, 0.00322249, 0.00249633, 0.00106075, 0.00113071,\n",
       "        0.00222575, 0.00360895, 0.007879  , 0.00158036, 0.00179174,\n",
       "        0.00199224, 0.00178058, 0.00189586, 0.00238526, 0.03869953,\n",
       "        0.0050629 , 0.00261755, 0.00599852, 0.00678579, 0.01613417,\n",
       "        0.00641078, 0.01347478, 0.00285046, 0.00683929, 0.00803808,\n",
       "        0.00161364, 0.00393504, 0.00520022, 0.00952299]),\n",
       " 'mean_score_time': array([0.02384281, 0.00502348, 0.00330915, 0.01176419, 0.01832218,\n",
       "        0.0510222 , 0.01400509, 0.00698581, 0.01104517, 0.00636029,\n",
       "        0.00478702, 0.00317326, 0.00461311, 0.00552564, 0.00828414,\n",
       "        0.01547208, 0.00559263, 0.01545997, 0.00724559, 0.00747342,\n",
       "        0.00470977, 0.004456  , 0.0145134 , 0.01785102, 0.00593753,\n",
       "        0.00340552, 0.00396099, 0.01042857, 0.00810776, 0.00402846,\n",
       "        0.01132202, 0.01410012, 0.004532  , 0.00517769, 0.00299621,\n",
       "        0.00404019, 0.00384288, 0.00247426, 0.00550141, 0.00426211,\n",
       "        0.00616603, 0.01157928, 0.00622296, 0.00745087, 0.00555596,\n",
       "        0.00453405, 0.01111727, 0.00788221, 0.00434651, 0.00277495,\n",
       "        0.00386071, 0.00368638, 0.00362859, 0.00449643, 0.00336318,\n",
       "        0.0027494 , 0.00351753, 0.00469766, 0.0051302 , 0.00550332,\n",
       "        0.00414014, 0.00518732, 0.00274334, 0.00317941, 0.00527306,\n",
       "        0.00408936, 0.00367966, 0.00325017, 0.0036459 , 0.00425196,\n",
       "        0.00744982, 0.00296783, 0.00253716, 0.00278335, 0.00608268,\n",
       "        0.00386658, 0.00303216, 0.00433602, 0.00362449, 0.00504184,\n",
       "        0.00394263, 0.00379114, 0.00309401, 0.00342903, 0.00294061,\n",
       "        0.00408921, 0.00351667, 0.00277729, 0.00350161, 0.00260601,\n",
       "        0.00252447, 0.00285888, 0.00244722, 0.00495558, 0.00304341,\n",
       "        0.00544567, 0.00305252, 0.00315795, 0.00286603, 0.00239577,\n",
       "        0.00358911, 0.00377779, 0.00361428, 0.00917153, 0.00713654,\n",
       "        0.00893192, 0.00494885, 0.00493283, 0.01726017, 0.00491972,\n",
       "        0.00588698, 0.0056468 , 0.06600833, 0.00944409, 0.00370774,\n",
       "        0.00500131, 0.0047195 , 0.00542412, 0.01002522, 0.00390568,\n",
       "        0.00389948, 0.00327582, 0.00827165, 0.01758738, 0.00475993,\n",
       "        0.00517683, 0.00971122, 0.00497332, 0.00262141, 0.01268206,\n",
       "        0.01587539, 0.0182126 , 0.0369782 , 0.0080235 , 0.0051476 ,\n",
       "        0.01029944, 0.00978904, 0.0067811 , 0.007618  , 0.00600166,\n",
       "        0.00335617, 0.00607319, 0.00501671, 0.02893205, 0.00765157,\n",
       "        0.00431199, 0.00416789, 0.00709176, 0.00981541, 0.00913906,\n",
       "        0.05474014, 0.01167622, 0.01754541, 0.00539775, 0.00984015,\n",
       "        0.00506611, 0.00490198, 0.00523901, 0.00428553, 0.00333767,\n",
       "        0.00514536, 0.00826664, 0.0171742 , 0.00757995, 0.00399342,\n",
       "        0.00740018, 0.04485335, 0.01407032, 0.00789285, 0.00850773,\n",
       "        0.00463672, 0.0032352 , 0.00439367, 0.00359178, 0.00880303,\n",
       "        0.06938105, 0.0078743 , 0.01190863, 0.00783606, 0.00496178,\n",
       "        0.00621271, 0.00702987, 0.01589899, 0.00395122, 0.0078455 ,\n",
       "        0.01226792, 0.00938582, 0.00815535, 0.00525179, 0.00578089,\n",
       "        0.00723047, 0.00849037, 0.00515294, 0.00461335, 0.00648332,\n",
       "        0.02079725, 0.01424346, 0.0123127 , 0.00905023, 0.00814781,\n",
       "        0.04897914, 0.00663018, 0.00732899, 0.0084753 , 0.01030164,\n",
       "        0.00539756, 0.0044003 , 0.00890393, 0.00562034, 0.00635123,\n",
       "        0.01288261, 0.0081676 , 0.01048074, 0.04459529, 0.01512246,\n",
       "        0.0050848 , 0.00824628, 0.00301147, 0.00344672, 0.00516415,\n",
       "        0.01137128, 0.00444598, 0.01200094, 0.04331698, 0.00456042,\n",
       "        0.01926007, 0.0406827 , 0.01636028, 0.01535378, 0.0131618 ,\n",
       "        0.10346961, 0.00294485, 0.00412192, 0.00971107, 0.01483564,\n",
       "        0.00622277, 0.00397363, 0.00337086, 0.00939674, 0.02385049,\n",
       "        0.00281315, 0.02891941, 0.00555964, 0.00416512, 0.00581031,\n",
       "        0.00327368, 0.01035147, 0.01021829, 0.00310135, 0.005264  ,\n",
       "        0.00608301, 0.00965924, 0.03220296, 0.0165164 , 0.01031699,\n",
       "        0.01005821, 0.02551045, 0.00516171, 0.00255818, 0.00385137,\n",
       "        0.00308065, 0.00451422, 0.00338078, 0.00509906, 0.00667138,\n",
       "        0.00461683, 0.0036273 , 0.00480232, 0.00425777, 0.00350852,\n",
       "        0.00273023, 0.00345097, 0.0035315 , 0.00696855, 0.00286579,\n",
       "        0.00676332, 0.00433917, 0.00354776, 0.00388017, 0.00473638,\n",
       "        0.00584016, 0.00259562, 0.00601754, 0.00459495, 0.00426245,\n",
       "        0.00463948, 0.00320573, 0.00614901, 0.00398326, 0.00314798,\n",
       "        0.00265059, 0.00430918, 0.00507293, 0.0031826 , 0.00356951,\n",
       "        0.00433059, 0.00318604, 0.00260625, 0.00375838, 0.00480781,\n",
       "        0.00316601, 0.00387797, 0.00374136, 0.00422425, 0.00292463,\n",
       "        0.00331783, 0.00458403, 0.00616693, 0.00293393, 0.00678806,\n",
       "        0.00378456, 0.01287241, 0.0116878 , 0.00796256, 0.00299602,\n",
       "        0.00438519, 0.00384007, 0.0042285 , 0.00423999, 0.00336299,\n",
       "        0.00349307, 0.00712214, 0.00435672, 0.00451455, 0.00266542,\n",
       "        0.00287123, 0.00615883, 0.00436864, 0.00428677, 0.0042201 ,\n",
       "        0.00226007, 0.00577087, 0.00339804, 0.00301404, 0.00221181,\n",
       "        0.00478911, 0.00422392, 0.00240436, 0.00381107, 0.0049417 ,\n",
       "        0.00371141, 0.00290561, 0.00362582, 0.00375214, 0.00311308,\n",
       "        0.00463738, 0.00464511, 0.00398774, 0.00303135, 0.0053287 ,\n",
       "        0.00310144, 0.00435243, 0.00343547, 0.00331826, 0.00520854,\n",
       "        0.00528293, 0.0059804 , 0.00557623, 0.00405984, 0.00492816,\n",
       "        0.00498261, 0.00347581, 0.00275321, 0.0034924 , 0.00280027,\n",
       "        0.00650954, 0.00832076, 0.00451875, 0.00335751, 0.00982065,\n",
       "        0.00493193, 0.00315051, 0.00415545, 0.00337787, 0.00770035,\n",
       "        0.00380993, 0.00299101, 0.00386014, 0.00473647, 0.00336981,\n",
       "        0.00359325, 0.00284686, 0.00437884, 0.00362096, 0.01228113,\n",
       "        0.00840006, 0.00339923, 0.00542226, 0.05041537, 0.00763106,\n",
       "        0.02183127, 0.00722575, 0.00461106, 0.00618134, 0.00383348,\n",
       "        0.00267348, 0.0040061 , 0.00371299, 0.00868721]),\n",
       " 'std_score_time': array([0.03375762, 0.0028605 , 0.00132865, 0.01332423, 0.02201078,\n",
       "        0.04809294, 0.00949102, 0.00290942, 0.00774248, 0.00313379,\n",
       "        0.00160899, 0.00075024, 0.00121099, 0.0022683 , 0.0021238 ,\n",
       "        0.01265686, 0.00222861, 0.01547257, 0.0050707 , 0.0071384 ,\n",
       "        0.00356011, 0.00118621, 0.0147396 , 0.01167396, 0.0032218 ,\n",
       "        0.00095548, 0.00131961, 0.00697553, 0.00373476, 0.00133857,\n",
       "        0.00628498, 0.01966149, 0.00306501, 0.00304817, 0.0008428 ,\n",
       "        0.00155801, 0.00343173, 0.00053595, 0.00275428, 0.00225914,\n",
       "        0.00639035, 0.00485109, 0.00758088, 0.00385688, 0.00222017,\n",
       "        0.00212123, 0.00647987, 0.00594789, 0.0016875 , 0.00121427,\n",
       "        0.0024697 , 0.00156552, 0.00124534, 0.00186211, 0.00159672,\n",
       "        0.00070539, 0.00144481, 0.00178493, 0.00319681, 0.0019223 ,\n",
       "        0.00183157, 0.00289547, 0.00036017, 0.00105007, 0.00296762,\n",
       "        0.00201051, 0.00196695, 0.00080976, 0.00197265, 0.00189301,\n",
       "        0.00713397, 0.00047227, 0.00047385, 0.00095429, 0.00536431,\n",
       "        0.00280279, 0.00066913, 0.00288359, 0.00105629, 0.00419885,\n",
       "        0.0034142 , 0.0023042 , 0.00070471, 0.0009109 , 0.0012334 ,\n",
       "        0.00087546, 0.00230859, 0.00086745, 0.00038362, 0.0007081 ,\n",
       "        0.00114521, 0.00082707, 0.00046881, 0.00258313, 0.00083683,\n",
       "        0.00308216, 0.00157822, 0.00119685, 0.00101538, 0.000417  ,\n",
       "        0.00246145, 0.0012691 , 0.00148957, 0.00548176, 0.00728431,\n",
       "        0.00589037, 0.00198946, 0.00211697, 0.00846427, 0.00197532,\n",
       "        0.00370669, 0.00203385, 0.11881724, 0.00940598, 0.00201568,\n",
       "        0.00263582, 0.00359734, 0.00352744, 0.00883233, 0.00147868,\n",
       "        0.00124602, 0.00140757, 0.00844263, 0.01172073, 0.00252126,\n",
       "        0.00486021, 0.00809708, 0.00220237, 0.00089372, 0.00869141,\n",
       "        0.00827188, 0.01356453, 0.06245759, 0.00677602, 0.00164792,\n",
       "        0.00727409, 0.00592437, 0.00376269, 0.00444236, 0.00386131,\n",
       "        0.0013005 , 0.00306547, 0.00214099, 0.01681428, 0.00546183,\n",
       "        0.00204807, 0.00353626, 0.00504394, 0.00680454, 0.00557019,\n",
       "        0.07797952, 0.00788551, 0.01687067, 0.00396566, 0.00578117,\n",
       "        0.00509555, 0.00198559, 0.00257066, 0.00173497, 0.00102941,\n",
       "        0.00338473, 0.00785282, 0.01645571, 0.00542765, 0.00236504,\n",
       "        0.0049171 , 0.0556533 , 0.01776484, 0.00650763, 0.00291645,\n",
       "        0.00315021, 0.00211627, 0.00263184, 0.00098486, 0.00560565,\n",
       "        0.06470008, 0.00610675, 0.00273586, 0.00460617, 0.00219838,\n",
       "        0.00309991, 0.00261762, 0.0186792 , 0.00267434, 0.00535538,\n",
       "        0.01101451, 0.00436903, 0.00433896, 0.00278024, 0.00466587,\n",
       "        0.00251524, 0.00628539, 0.00369094, 0.00170232, 0.00361211,\n",
       "        0.0139569 , 0.01610149, 0.00874419, 0.00808591, 0.00193213,\n",
       "        0.07115552, 0.00371841, 0.00624626, 0.00556493, 0.00573909,\n",
       "        0.00196753, 0.00144168, 0.00733884, 0.00240033, 0.00182627,\n",
       "        0.01379743, 0.00289995, 0.00467233, 0.02924128, 0.01053189,\n",
       "        0.00430325, 0.00589357, 0.00107143, 0.00110826, 0.00350149,\n",
       "        0.00656519, 0.00246727, 0.00718422, 0.07280218, 0.00194147,\n",
       "        0.01736619, 0.04360691, 0.00786743, 0.01104557, 0.00943924,\n",
       "        0.17848165, 0.00124086, 0.00185535, 0.00529198, 0.01323735,\n",
       "        0.00514385, 0.00187872, 0.00089909, 0.00462433, 0.03100081,\n",
       "        0.00059554, 0.04025017, 0.00138566, 0.00310584, 0.00316348,\n",
       "        0.00118878, 0.00630448, 0.00571915, 0.00158095, 0.00256106,\n",
       "        0.00588961, 0.00498935, 0.0211815 , 0.0197309 , 0.00613579,\n",
       "        0.00515617, 0.01138165, 0.00413823, 0.00096921, 0.00192666,\n",
       "        0.00141701, 0.00159531, 0.00094351, 0.00280532, 0.00410354,\n",
       "        0.00370143, 0.00121045, 0.00160792, 0.00157933, 0.00085527,\n",
       "        0.00091726, 0.00183792, 0.00164418, 0.00420425, 0.00101941,\n",
       "        0.00320486, 0.00219174, 0.0010203 , 0.00127457, 0.00316829,\n",
       "        0.0062823 , 0.00102911, 0.00334041, 0.00301466, 0.00093847,\n",
       "        0.00196311, 0.00109557, 0.00268135, 0.00156646, 0.00074372,\n",
       "        0.00098962, 0.00060527, 0.00098838, 0.00112964, 0.00119243,\n",
       "        0.00203788, 0.00075133, 0.00049241, 0.002072  , 0.00310497,\n",
       "        0.00158164, 0.00081384, 0.00118635, 0.00230437, 0.00087986,\n",
       "        0.00108217, 0.00292064, 0.00319169, 0.00058264, 0.0068279 ,\n",
       "        0.00203547, 0.00792587, 0.01225401, 0.006185  , 0.00066811,\n",
       "        0.00217422, 0.00124601, 0.0019003 , 0.00207581, 0.00108702,\n",
       "        0.0028463 , 0.00623582, 0.00119186, 0.00196643, 0.00138452,\n",
       "        0.00060804, 0.00498062, 0.00187491, 0.00305469, 0.00092829,\n",
       "        0.00056614, 0.00274225, 0.00158065, 0.00137388, 0.00027014,\n",
       "        0.0017001 , 0.00144496, 0.00052478, 0.00150445, 0.0029977 ,\n",
       "        0.00164622, 0.00093619, 0.00101606, 0.00135262, 0.00151762,\n",
       "        0.00257635, 0.0025767 , 0.00172951, 0.00103838, 0.00259042,\n",
       "        0.00164224, 0.0021853 , 0.00079684, 0.0007127 , 0.00384143,\n",
       "        0.00344326, 0.00197127, 0.00126047, 0.00253972, 0.00247358,\n",
       "        0.00321532, 0.00138923, 0.00051663, 0.001127  , 0.00076077,\n",
       "        0.00624656, 0.00708547, 0.00225544, 0.00112404, 0.01311712,\n",
       "        0.00283946, 0.00112525, 0.00269424, 0.00124843, 0.00548692,\n",
       "        0.00155564, 0.00091799, 0.00075238, 0.0029721 , 0.00169077,\n",
       "        0.00256476, 0.0010646 , 0.00104424, 0.00187393, 0.01596078,\n",
       "        0.00697955, 0.00193143, 0.00513748, 0.09119416, 0.00236451,\n",
       "        0.01864083, 0.00449421, 0.00229396, 0.00651167, 0.00126136,\n",
       "        0.00058263, 0.00142504, 0.00312967, 0.01105947]),\n",
       " 'param_max_depth': masked_array(data=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,\n",
       "                    17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
       "                    31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44,\n",
       "                    45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58,\n",
       "                    59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
       "                    73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86,\n",
       "                    87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99,\n",
       "                    100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110,\n",
       "                    111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121,\n",
       "                    122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132,\n",
       "                    133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
       "                    144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154,\n",
       "                    155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165,\n",
       "                    166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176,\n",
       "                    177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187,\n",
       "                    188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198,\n",
       "                    199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
       "                    210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "                    221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231,\n",
       "                    232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242,\n",
       "                    243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
       "                    254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264,\n",
       "                    265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275,\n",
       "                    276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286,\n",
       "                    287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
       "                    298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308,\n",
       "                    309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319,\n",
       "                    320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330,\n",
       "                    331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341,\n",
       "                    342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352,\n",
       "                    353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "                    364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374,\n",
       "                    375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385,\n",
       "                    386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396,\n",
       "                    397, 398, 399],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 1},\n",
       "  {'max_depth': 2},\n",
       "  {'max_depth': 3},\n",
       "  {'max_depth': 4},\n",
       "  {'max_depth': 5},\n",
       "  {'max_depth': 6},\n",
       "  {'max_depth': 7},\n",
       "  {'max_depth': 8},\n",
       "  {'max_depth': 9},\n",
       "  {'max_depth': 10},\n",
       "  {'max_depth': 11},\n",
       "  {'max_depth': 12},\n",
       "  {'max_depth': 13},\n",
       "  {'max_depth': 14},\n",
       "  {'max_depth': 15},\n",
       "  {'max_depth': 16},\n",
       "  {'max_depth': 17},\n",
       "  {'max_depth': 18},\n",
       "  {'max_depth': 19},\n",
       "  {'max_depth': 20},\n",
       "  {'max_depth': 21},\n",
       "  {'max_depth': 22},\n",
       "  {'max_depth': 23},\n",
       "  {'max_depth': 24},\n",
       "  {'max_depth': 25},\n",
       "  {'max_depth': 26},\n",
       "  {'max_depth': 27},\n",
       "  {'max_depth': 28},\n",
       "  {'max_depth': 29},\n",
       "  {'max_depth': 30},\n",
       "  {'max_depth': 31},\n",
       "  {'max_depth': 32},\n",
       "  {'max_depth': 33},\n",
       "  {'max_depth': 34},\n",
       "  {'max_depth': 35},\n",
       "  {'max_depth': 36},\n",
       "  {'max_depth': 37},\n",
       "  {'max_depth': 38},\n",
       "  {'max_depth': 39},\n",
       "  {'max_depth': 40},\n",
       "  {'max_depth': 41},\n",
       "  {'max_depth': 42},\n",
       "  {'max_depth': 43},\n",
       "  {'max_depth': 44},\n",
       "  {'max_depth': 45},\n",
       "  {'max_depth': 46},\n",
       "  {'max_depth': 47},\n",
       "  {'max_depth': 48},\n",
       "  {'max_depth': 49},\n",
       "  {'max_depth': 50},\n",
       "  {'max_depth': 51},\n",
       "  {'max_depth': 52},\n",
       "  {'max_depth': 53},\n",
       "  {'max_depth': 54},\n",
       "  {'max_depth': 55},\n",
       "  {'max_depth': 56},\n",
       "  {'max_depth': 57},\n",
       "  {'max_depth': 58},\n",
       "  {'max_depth': 59},\n",
       "  {'max_depth': 60},\n",
       "  {'max_depth': 61},\n",
       "  {'max_depth': 62},\n",
       "  {'max_depth': 63},\n",
       "  {'max_depth': 64},\n",
       "  {'max_depth': 65},\n",
       "  {'max_depth': 66},\n",
       "  {'max_depth': 67},\n",
       "  {'max_depth': 68},\n",
       "  {'max_depth': 69},\n",
       "  {'max_depth': 70},\n",
       "  {'max_depth': 71},\n",
       "  {'max_depth': 72},\n",
       "  {'max_depth': 73},\n",
       "  {'max_depth': 74},\n",
       "  {'max_depth': 75},\n",
       "  {'max_depth': 76},\n",
       "  {'max_depth': 77},\n",
       "  {'max_depth': 78},\n",
       "  {'max_depth': 79},\n",
       "  {'max_depth': 80},\n",
       "  {'max_depth': 81},\n",
       "  {'max_depth': 82},\n",
       "  {'max_depth': 83},\n",
       "  {'max_depth': 84},\n",
       "  {'max_depth': 85},\n",
       "  {'max_depth': 86},\n",
       "  {'max_depth': 87},\n",
       "  {'max_depth': 88},\n",
       "  {'max_depth': 89},\n",
       "  {'max_depth': 90},\n",
       "  {'max_depth': 91},\n",
       "  {'max_depth': 92},\n",
       "  {'max_depth': 93},\n",
       "  {'max_depth': 94},\n",
       "  {'max_depth': 95},\n",
       "  {'max_depth': 96},\n",
       "  {'max_depth': 97},\n",
       "  {'max_depth': 98},\n",
       "  {'max_depth': 99},\n",
       "  {'max_depth': 100},\n",
       "  {'max_depth': 101},\n",
       "  {'max_depth': 102},\n",
       "  {'max_depth': 103},\n",
       "  {'max_depth': 104},\n",
       "  {'max_depth': 105},\n",
       "  {'max_depth': 106},\n",
       "  {'max_depth': 107},\n",
       "  {'max_depth': 108},\n",
       "  {'max_depth': 109},\n",
       "  {'max_depth': 110},\n",
       "  {'max_depth': 111},\n",
       "  {'max_depth': 112},\n",
       "  {'max_depth': 113},\n",
       "  {'max_depth': 114},\n",
       "  {'max_depth': 115},\n",
       "  {'max_depth': 116},\n",
       "  {'max_depth': 117},\n",
       "  {'max_depth': 118},\n",
       "  {'max_depth': 119},\n",
       "  {'max_depth': 120},\n",
       "  {'max_depth': 121},\n",
       "  {'max_depth': 122},\n",
       "  {'max_depth': 123},\n",
       "  {'max_depth': 124},\n",
       "  {'max_depth': 125},\n",
       "  {'max_depth': 126},\n",
       "  {'max_depth': 127},\n",
       "  {'max_depth': 128},\n",
       "  {'max_depth': 129},\n",
       "  {'max_depth': 130},\n",
       "  {'max_depth': 131},\n",
       "  {'max_depth': 132},\n",
       "  {'max_depth': 133},\n",
       "  {'max_depth': 134},\n",
       "  {'max_depth': 135},\n",
       "  {'max_depth': 136},\n",
       "  {'max_depth': 137},\n",
       "  {'max_depth': 138},\n",
       "  {'max_depth': 139},\n",
       "  {'max_depth': 140},\n",
       "  {'max_depth': 141},\n",
       "  {'max_depth': 142},\n",
       "  {'max_depth': 143},\n",
       "  {'max_depth': 144},\n",
       "  {'max_depth': 145},\n",
       "  {'max_depth': 146},\n",
       "  {'max_depth': 147},\n",
       "  {'max_depth': 148},\n",
       "  {'max_depth': 149},\n",
       "  {'max_depth': 150},\n",
       "  {'max_depth': 151},\n",
       "  {'max_depth': 152},\n",
       "  {'max_depth': 153},\n",
       "  {'max_depth': 154},\n",
       "  {'max_depth': 155},\n",
       "  {'max_depth': 156},\n",
       "  {'max_depth': 157},\n",
       "  {'max_depth': 158},\n",
       "  {'max_depth': 159},\n",
       "  {'max_depth': 160},\n",
       "  {'max_depth': 161},\n",
       "  {'max_depth': 162},\n",
       "  {'max_depth': 163},\n",
       "  {'max_depth': 164},\n",
       "  {'max_depth': 165},\n",
       "  {'max_depth': 166},\n",
       "  {'max_depth': 167},\n",
       "  {'max_depth': 168},\n",
       "  {'max_depth': 169},\n",
       "  {'max_depth': 170},\n",
       "  {'max_depth': 171},\n",
       "  {'max_depth': 172},\n",
       "  {'max_depth': 173},\n",
       "  {'max_depth': 174},\n",
       "  {'max_depth': 175},\n",
       "  {'max_depth': 176},\n",
       "  {'max_depth': 177},\n",
       "  {'max_depth': 178},\n",
       "  {'max_depth': 179},\n",
       "  {'max_depth': 180},\n",
       "  {'max_depth': 181},\n",
       "  {'max_depth': 182},\n",
       "  {'max_depth': 183},\n",
       "  {'max_depth': 184},\n",
       "  {'max_depth': 185},\n",
       "  {'max_depth': 186},\n",
       "  {'max_depth': 187},\n",
       "  {'max_depth': 188},\n",
       "  {'max_depth': 189},\n",
       "  {'max_depth': 190},\n",
       "  {'max_depth': 191},\n",
       "  {'max_depth': 192},\n",
       "  {'max_depth': 193},\n",
       "  {'max_depth': 194},\n",
       "  {'max_depth': 195},\n",
       "  {'max_depth': 196},\n",
       "  {'max_depth': 197},\n",
       "  {'max_depth': 198},\n",
       "  {'max_depth': 199},\n",
       "  {'max_depth': 200},\n",
       "  {'max_depth': 201},\n",
       "  {'max_depth': 202},\n",
       "  {'max_depth': 203},\n",
       "  {'max_depth': 204},\n",
       "  {'max_depth': 205},\n",
       "  {'max_depth': 206},\n",
       "  {'max_depth': 207},\n",
       "  {'max_depth': 208},\n",
       "  {'max_depth': 209},\n",
       "  {'max_depth': 210},\n",
       "  {'max_depth': 211},\n",
       "  {'max_depth': 212},\n",
       "  {'max_depth': 213},\n",
       "  {'max_depth': 214},\n",
       "  {'max_depth': 215},\n",
       "  {'max_depth': 216},\n",
       "  {'max_depth': 217},\n",
       "  {'max_depth': 218},\n",
       "  {'max_depth': 219},\n",
       "  {'max_depth': 220},\n",
       "  {'max_depth': 221},\n",
       "  {'max_depth': 222},\n",
       "  {'max_depth': 223},\n",
       "  {'max_depth': 224},\n",
       "  {'max_depth': 225},\n",
       "  {'max_depth': 226},\n",
       "  {'max_depth': 227},\n",
       "  {'max_depth': 228},\n",
       "  {'max_depth': 229},\n",
       "  {'max_depth': 230},\n",
       "  {'max_depth': 231},\n",
       "  {'max_depth': 232},\n",
       "  {'max_depth': 233},\n",
       "  {'max_depth': 234},\n",
       "  {'max_depth': 235},\n",
       "  {'max_depth': 236},\n",
       "  {'max_depth': 237},\n",
       "  {'max_depth': 238},\n",
       "  {'max_depth': 239},\n",
       "  {'max_depth': 240},\n",
       "  {'max_depth': 241},\n",
       "  {'max_depth': 242},\n",
       "  {'max_depth': 243},\n",
       "  {'max_depth': 244},\n",
       "  {'max_depth': 245},\n",
       "  {'max_depth': 246},\n",
       "  {'max_depth': 247},\n",
       "  {'max_depth': 248},\n",
       "  {'max_depth': 249},\n",
       "  {'max_depth': 250},\n",
       "  {'max_depth': 251},\n",
       "  {'max_depth': 252},\n",
       "  {'max_depth': 253},\n",
       "  {'max_depth': 254},\n",
       "  {'max_depth': 255},\n",
       "  {'max_depth': 256},\n",
       "  {'max_depth': 257},\n",
       "  {'max_depth': 258},\n",
       "  {'max_depth': 259},\n",
       "  {'max_depth': 260},\n",
       "  {'max_depth': 261},\n",
       "  {'max_depth': 262},\n",
       "  {'max_depth': 263},\n",
       "  {'max_depth': 264},\n",
       "  {'max_depth': 265},\n",
       "  {'max_depth': 266},\n",
       "  {'max_depth': 267},\n",
       "  {'max_depth': 268},\n",
       "  {'max_depth': 269},\n",
       "  {'max_depth': 270},\n",
       "  {'max_depth': 271},\n",
       "  {'max_depth': 272},\n",
       "  {'max_depth': 273},\n",
       "  {'max_depth': 274},\n",
       "  {'max_depth': 275},\n",
       "  {'max_depth': 276},\n",
       "  {'max_depth': 277},\n",
       "  {'max_depth': 278},\n",
       "  {'max_depth': 279},\n",
       "  {'max_depth': 280},\n",
       "  {'max_depth': 281},\n",
       "  {'max_depth': 282},\n",
       "  {'max_depth': 283},\n",
       "  {'max_depth': 284},\n",
       "  {'max_depth': 285},\n",
       "  {'max_depth': 286},\n",
       "  {'max_depth': 287},\n",
       "  {'max_depth': 288},\n",
       "  {'max_depth': 289},\n",
       "  {'max_depth': 290},\n",
       "  {'max_depth': 291},\n",
       "  {'max_depth': 292},\n",
       "  {'max_depth': 293},\n",
       "  {'max_depth': 294},\n",
       "  {'max_depth': 295},\n",
       "  {'max_depth': 296},\n",
       "  {'max_depth': 297},\n",
       "  {'max_depth': 298},\n",
       "  {'max_depth': 299},\n",
       "  {'max_depth': 300},\n",
       "  {'max_depth': 301},\n",
       "  {'max_depth': 302},\n",
       "  {'max_depth': 303},\n",
       "  {'max_depth': 304},\n",
       "  {'max_depth': 305},\n",
       "  {'max_depth': 306},\n",
       "  {'max_depth': 307},\n",
       "  {'max_depth': 308},\n",
       "  {'max_depth': 309},\n",
       "  {'max_depth': 310},\n",
       "  {'max_depth': 311},\n",
       "  {'max_depth': 312},\n",
       "  {'max_depth': 313},\n",
       "  {'max_depth': 314},\n",
       "  {'max_depth': 315},\n",
       "  {'max_depth': 316},\n",
       "  {'max_depth': 317},\n",
       "  {'max_depth': 318},\n",
       "  {'max_depth': 319},\n",
       "  {'max_depth': 320},\n",
       "  {'max_depth': 321},\n",
       "  {'max_depth': 322},\n",
       "  {'max_depth': 323},\n",
       "  {'max_depth': 324},\n",
       "  {'max_depth': 325},\n",
       "  {'max_depth': 326},\n",
       "  {'max_depth': 327},\n",
       "  {'max_depth': 328},\n",
       "  {'max_depth': 329},\n",
       "  {'max_depth': 330},\n",
       "  {'max_depth': 331},\n",
       "  {'max_depth': 332},\n",
       "  {'max_depth': 333},\n",
       "  {'max_depth': 334},\n",
       "  {'max_depth': 335},\n",
       "  {'max_depth': 336},\n",
       "  {'max_depth': 337},\n",
       "  {'max_depth': 338},\n",
       "  {'max_depth': 339},\n",
       "  {'max_depth': 340},\n",
       "  {'max_depth': 341},\n",
       "  {'max_depth': 342},\n",
       "  {'max_depth': 343},\n",
       "  {'max_depth': 344},\n",
       "  {'max_depth': 345},\n",
       "  {'max_depth': 346},\n",
       "  {'max_depth': 347},\n",
       "  {'max_depth': 348},\n",
       "  {'max_depth': 349},\n",
       "  {'max_depth': 350},\n",
       "  {'max_depth': 351},\n",
       "  {'max_depth': 352},\n",
       "  {'max_depth': 353},\n",
       "  {'max_depth': 354},\n",
       "  {'max_depth': 355},\n",
       "  {'max_depth': 356},\n",
       "  {'max_depth': 357},\n",
       "  {'max_depth': 358},\n",
       "  {'max_depth': 359},\n",
       "  {'max_depth': 360},\n",
       "  {'max_depth': 361},\n",
       "  {'max_depth': 362},\n",
       "  {'max_depth': 363},\n",
       "  {'max_depth': 364},\n",
       "  {'max_depth': 365},\n",
       "  {'max_depth': 366},\n",
       "  {'max_depth': 367},\n",
       "  {'max_depth': 368},\n",
       "  {'max_depth': 369},\n",
       "  {'max_depth': 370},\n",
       "  {'max_depth': 371},\n",
       "  {'max_depth': 372},\n",
       "  {'max_depth': 373},\n",
       "  {'max_depth': 374},\n",
       "  {'max_depth': 375},\n",
       "  {'max_depth': 376},\n",
       "  {'max_depth': 377},\n",
       "  {'max_depth': 378},\n",
       "  {'max_depth': 379},\n",
       "  {'max_depth': 380},\n",
       "  {'max_depth': 381},\n",
       "  {'max_depth': 382},\n",
       "  {'max_depth': 383},\n",
       "  {'max_depth': 384},\n",
       "  {'max_depth': 385},\n",
       "  {'max_depth': 386},\n",
       "  {'max_depth': 387},\n",
       "  {'max_depth': 388},\n",
       "  {'max_depth': 389},\n",
       "  {'max_depth': 390},\n",
       "  {'max_depth': 391},\n",
       "  {'max_depth': 392},\n",
       "  {'max_depth': 393},\n",
       "  {'max_depth': 394},\n",
       "  {'max_depth': 395},\n",
       "  {'max_depth': 396},\n",
       "  {'max_depth': 397},\n",
       "  {'max_depth': 398},\n",
       "  {'max_depth': 399}],\n",
       " 'split0_test_score': array([0.46580786, 0.53598376, 0.67570413, 0.70626423, 0.66314682,\n",
       "        0.67721581, 0.65017126, 0.64304009, 0.68616749, 0.72636518,\n",
       "        0.73496081, 0.71619866, 0.72227371, 0.71750595, 0.70681447,\n",
       "        0.66716338, 0.71655005, 0.69854948, 0.70237012, 0.70894332,\n",
       "        0.67992984, 0.74475583, 0.65753739, 0.66960243, 0.66690301,\n",
       "        0.73400331, 0.68454755, 0.69264582, 0.72774362, 0.72421259,\n",
       "        0.66560378, 0.6573275 , 0.75768701, 0.69281055, 0.70470554,\n",
       "        0.69075676, 0.68780758, 0.72267158, 0.74446091, 0.69081255,\n",
       "        0.67099465, 0.69591117, 0.70796292, 0.68559438, 0.67815768,\n",
       "        0.65734875, 0.73229491, 0.70320173, 0.69918182, 0.71246107,\n",
       "        0.73852271, 0.74592221, 0.67150212, 0.69229777, 0.74161004,\n",
       "        0.73779206, 0.70002141, 0.738807  , 0.68178702, 0.7073226 ,\n",
       "        0.74790958, 0.73491463, 0.67803281, 0.72092864, 0.69168402,\n",
       "        0.73087612, 0.68608591, 0.667267  , 0.65530824, 0.68648976,\n",
       "        0.71263377, 0.72064701, 0.65589276, 0.69360497, 0.71996684,\n",
       "        0.72054073, 0.70700377, 0.73987774, 0.70277928, 0.6756283 ,\n",
       "        0.71516846, 0.6920427 , 0.69175044, 0.73218863, 0.71307747,\n",
       "        0.69686234, 0.72483165, 0.6594238 , 0.65264868, 0.68744359,\n",
       "        0.66275822, 0.6919258 , 0.69649569, 0.68964617, 0.71531725,\n",
       "        0.6963097 , 0.66854498, 0.7040174 , 0.65007944, 0.70539103,\n",
       "        0.72740619, 0.70587458, 0.66893554, 0.74550773, 0.74558744,\n",
       "        0.70003203, 0.74017531, 0.72462441, 0.68524898, 0.73672133,\n",
       "        0.73969707, 0.68902445, 0.6541764 , 0.71758625, 0.70302903,\n",
       "        0.75414801, 0.70968991, 0.69648771, 0.69600681, 0.73358351,\n",
       "        0.73474458, 0.74471332, 0.73743072, 0.73620854, 0.65090308,\n",
       "        0.73426103, 0.757145  , 0.70748201, 0.74019922, 0.73956422,\n",
       "        0.72612291, 0.72883827, 0.74176946, 0.68617624, 0.67873689,\n",
       "        0.64741456, 0.71947531, 0.7309638 , 0.69416292, 0.6881025 ,\n",
       "        0.70294401, 0.74708594, 0.74520219, 0.69260065, 0.7283494 ,\n",
       "        0.68531275, 0.73335502, 0.70411836, 0.71706549, 0.7107819 ,\n",
       "        0.69685703, 0.72513719, 0.7396705 , 0.73528659, 0.69711475,\n",
       "        0.71978086, 0.75118289, 0.69832098, 0.69058937, 0.70661586,\n",
       "        0.70225853, 0.71151521, 0.70593304, 0.72375294, 0.68655352,\n",
       "        0.68765348, 0.6764732 , 0.69820142, 0.69487497, 0.69948471,\n",
       "        0.70281913, 0.69556577, 0.72230492, 0.67673358, 0.72812622,\n",
       "        0.73341878, 0.7475323 , 0.68074551, 0.70167401, 0.65710432,\n",
       "        0.69533196, 0.72520096, 0.69658336, 0.7133405 , 0.68268506,\n",
       "        0.67071567, 0.7230276 , 0.72947327, 0.69990185, 0.70684436,\n",
       "        0.72275128, 0.71709206, 0.6856422 , 0.73670007, 0.73147393,\n",
       "        0.73958282, 0.70831097, 0.69033165, 0.68989857, 0.73828359,\n",
       "        0.7347313 , 0.68578302, 0.67821348, 0.72766657, 0.74369838,\n",
       "        0.66599966, 0.72800134, 0.71816811, 0.74774485, 0.69175576,\n",
       "        0.7031114 , 0.7026119 , 0.7130137 , 0.72751513, 0.68210054,\n",
       "        0.66027401, 0.7372102 , 0.72288147, 0.73362337, 0.64651652,\n",
       "        0.64432989, 0.7054973 , 0.65014586, 0.65906777, 0.74780862,\n",
       "        0.70120905, 0.69641598, 0.66509631, 0.71525879, 0.67706835,\n",
       "        0.64036312, 0.70166338, 0.71173307, 0.68053296, 0.7010576 ,\n",
       "        0.70878656, 0.69729276, 0.73427431, 0.73802321, 0.71709738,\n",
       "        0.74021516, 0.73025706, 0.70538837, 0.65650651, 0.71269222,\n",
       "        0.66216042, 0.73575952, 0.6870238 , 0.73458251, 0.7133405 ,\n",
       "        0.67085118, 0.7344231 , 0.69400882, 0.64271183, 0.66883458,\n",
       "        0.70656007, 0.68894209, 0.7248715 , 0.7402683 , 0.7013844 ,\n",
       "        0.72965128, 0.73211955, 0.73179541, 0.72985852, 0.71937966,\n",
       "        0.70409977, 0.69101979, 0.68541371, 0.74398532, 0.64882007,\n",
       "        0.70784601, 0.69261925, 0.70717913, 0.73351709, 0.65311629,\n",
       "        0.72778348, 0.6808757 , 0.72272472, 0.72827235, 0.67538121,\n",
       "        0.71641189, 0.70031101, 0.68789261, 0.7514725 , 0.69000219,\n",
       "        0.68636488, 0.73943935, 0.68913073, 0.70328675, 0.6651335 ,\n",
       "        0.71692733, 0.71065702, 0.71346538, 0.72115714, 0.65026011,\n",
       "        0.70615356, 0.74481428, 0.67604809, 0.7541055 , 0.67976511,\n",
       "        0.72093661, 0.72964863, 0.7270236 , 0.69231902, 0.7355018 ,\n",
       "        0.74111851, 0.74337423, 0.72647627, 0.67305376, 0.66997971,\n",
       "        0.72351913, 0.73782394, 0.67402087, 0.66618298, 0.73092129,\n",
       "        0.69879126, 0.69547543, 0.71703095, 0.68722306, 0.72528598,\n",
       "        0.6545218 , 0.69652491, 0.721468  , 0.68117328, 0.70094601,\n",
       "        0.72514516, 0.7037464 , 0.67671498, 0.66830054, 0.65429065,\n",
       "        0.696865  , 0.72780739, 0.73781863, 0.70598086, 0.69967335,\n",
       "        0.66420624, 0.73182729, 0.75311181, 0.74522079, 0.74077843,\n",
       "        0.68171529, 0.69631767, 0.69618483, 0.72732914, 0.65629396,\n",
       "        0.70080254, 0.72712456, 0.72657989, 0.70449033, 0.70357901,\n",
       "        0.64422627, 0.71882968, 0.74519953, 0.68618156, 0.67771132,\n",
       "        0.72939622, 0.74501089, 0.74952764, 0.7260432 , 0.65848857,\n",
       "        0.64467794, 0.6631541 , 0.6665151 , 0.70702503, 0.70937905,\n",
       "        0.73927993, 0.69300716, 0.73736164, 0.6940407 , 0.70870419,\n",
       "        0.71907412, 0.67996173, 0.75380792, 0.73557885, 0.71558294,\n",
       "        0.71449095, 0.71366465, 0.71023989, 0.73408833, 0.70075737,\n",
       "        0.65168156, 0.70781944, 0.73038725, 0.73853865, 0.75328451,\n",
       "        0.68513208, 0.7414028 , 0.73269876, 0.75334296, 0.68840805,\n",
       "        0.69781617, 0.74580531, 0.70206723, 0.70464443, 0.68438814,\n",
       "        0.69593242, 0.69818814, 0.65411263, 0.73521486]),\n",
       " 'split1_test_score': array([0.42063054, 0.68823943, 0.82202922, 0.82465247, 0.7743005 ,\n",
       "        0.75679093, 0.77118461, 0.76101779, 0.76130967, 0.75199187,\n",
       "        0.73276825, 0.74872576, 0.77277102, 0.75364066, 0.79094432,\n",
       "        0.74590416, 0.77453671, 0.78536262, 0.73545037, 0.74994549,\n",
       "        0.77473863, 0.7518724 , 0.77432614, 0.74720223, 0.7657906 ,\n",
       "        0.74445321, 0.75385701, 0.7907049 , 0.75450893, 0.75899448,\n",
       "        0.77850592, 0.74356764, 0.76145505, 0.74835608, 0.7517051 ,\n",
       "        0.7610714 , 0.74181092, 0.72974463, 0.74802723, 0.79172316,\n",
       "        0.76857136, 0.74411571, 0.76795117, 0.74412725, 0.77260115,\n",
       "        0.74752243, 0.73493114, 0.77036846, 0.74806473, 0.74526378,\n",
       "        0.77499825, 0.78884145, 0.73628114, 0.73741478, 0.78928856,\n",
       "        0.75809449, 0.77046943, 0.73534941, 0.74035708, 0.7260783 ,\n",
       "        0.72897156, 0.75438201, 0.75772237, 0.74564743, 0.74805896,\n",
       "        0.73344557, 0.77361076, 0.74498686, 0.75825603, 0.74461186,\n",
       "        0.79511834, 0.74081573, 0.75936948, 0.79000394, 0.75915025,\n",
       "        0.71816584, 0.75717718, 0.7376744 , 0.79289431, 0.76433676,\n",
       "        0.74542244, 0.75851852, 0.73203789, 0.73178116, 0.725911  ,\n",
       "        0.73322634, 0.74868204, 0.75000607, 0.74402917, 0.72673888,\n",
       "        0.72671868, 0.74030515, 0.76091851, 0.79100778, 0.73022059,\n",
       "        0.77624728, 0.74281764, 0.76424734, 0.78814914, 0.75058587,\n",
       "        0.7383667 , 0.74980414, 0.76672233, 0.76315985, 0.75718584,\n",
       "        0.75049068, 0.77520306, 0.76211851, 0.73934458, 0.72356293,\n",
       "        0.74307149, 0.76579349, 0.72564273, 0.75245509, 0.76543291,\n",
       "        0.74936569, 0.77054154, 0.7861905 , 0.76477811, 0.73549364,\n",
       "        0.79644526, 0.76141178, 0.75196471, 0.75832526, 0.77008289,\n",
       "        0.76966174, 0.75876083, 0.74179361, 0.75743391, 0.75729545,\n",
       "        0.7800838 , 0.75249259, 0.74946665, 0.73061578, 0.76132813,\n",
       "        0.75117433, 0.75092049, 0.73541287, 0.77550594, 0.75401566,\n",
       "        0.77116173, 0.748183  , 0.76883963, 0.72445716, 0.73203212,\n",
       "        0.76863193, 0.7758694 , 0.75087433, 0.72606965, 0.75419451,\n",
       "        0.7577743 , 0.79403661, 0.78241456, 0.76678867, 0.73197154,\n",
       "        0.80583463, 0.7458032 , 0.76740598, 0.7592339 , 0.75789257,\n",
       "        0.78767318, 0.74207918, 0.71723412, 0.76496272, 0.75197913,\n",
       "        0.76618291, 0.73710902, 0.76460792, 0.74740416, 0.74584359,\n",
       "        0.72083698, 0.77072615, 0.74483686, 0.76322908, 0.77003962,\n",
       "        0.76892905, 0.7648358 , 0.73003598, 0.74204745, 0.76839251,\n",
       "        0.76553964, 0.75038972, 0.77683862, 0.77980111, 0.75597719,\n",
       "        0.73999073, 0.75244355, 0.7544022 , 0.76538676, 0.75812333,\n",
       "        0.77093673, 0.7655483 , 0.73666479, 0.74603974, 0.74954453,\n",
       "        0.74152246, 0.74810511, 0.77631363, 0.76499157, 0.75474547,\n",
       "        0.73783882, 0.73268404, 0.73322346, 0.74621282, 0.73047155,\n",
       "        0.74987338, 0.79064432, 0.75613296, 0.74375514, 0.74063111,\n",
       "        0.76015121, 0.73972823, 0.73971093, 0.73606479, 0.72282736,\n",
       "        0.76182716, 0.74429456, 0.75133875, 0.73450422, 0.73858593,\n",
       "        0.72409658, 0.75388297, 0.72772253, 0.79090971, 0.76911943,\n",
       "        0.76677714, 0.74211668, 0.78084533, 0.74536474, 0.75303778,\n",
       "        0.74088784, 0.75893679, 0.74205611, 0.7411013 , 0.74711858,\n",
       "        0.7409715 , 0.76375119, 0.77889534, 0.73609075, 0.74219168,\n",
       "        0.75893968, 0.77076942, 0.74210515, 0.72935809, 0.73066193,\n",
       "        0.75275221, 0.76768867, 0.7590291 , 0.75673584, 0.77188576,\n",
       "        0.73362442, 0.75838872, 0.75516662, 0.73359557, 0.75384259,\n",
       "        0.75049356, 0.76277043, 0.75628873, 0.7611839 , 0.73101385,\n",
       "        0.76887713, 0.74805608, 0.77510209, 0.72990328, 0.72599753,\n",
       "        0.77369152, 0.74315226, 0.75652815, 0.73128789, 0.75127818,\n",
       "        0.74512244, 0.76559445, 0.73003021, 0.77943476, 0.75611277,\n",
       "        0.76439446, 0.75898871, 0.72204275, 0.73893785, 0.71983314,\n",
       "        0.75799353, 0.75485508, 0.76229158, 0.77060789, 0.75413682,\n",
       "        0.74212822, 0.7881578 , 0.77532421, 0.71436971, 0.75507143,\n",
       "        0.76383196, 0.78488089, 0.7670079 , 0.77330787, 0.78977029,\n",
       "        0.72822156, 0.7497753 , 0.76018871, 0.74779646, 0.75563104,\n",
       "        0.74791762, 0.75153491, 0.74533013, 0.72597734, 0.73658979,\n",
       "        0.73344557, 0.76503484, 0.73837824, 0.75232817, 0.76279927,\n",
       "        0.77567613, 0.77328768, 0.73369653, 0.76742617, 0.75116279,\n",
       "        0.77405787, 0.76009352, 0.76039351, 0.7229889 , 0.74535321,\n",
       "        0.75393201, 0.72592542, 0.77860977, 0.73952631, 0.73197731,\n",
       "        0.78506551, 0.76827424, 0.75894256, 0.75654257, 0.74075804,\n",
       "        0.74940607, 0.79614526, 0.73708594, 0.77254057, 0.74712724,\n",
       "        0.75007818, 0.73489364, 0.72796772, 0.73894651, 0.75019645,\n",
       "        0.76737425, 0.76716944, 0.76626368, 0.74327629, 0.77166653,\n",
       "        0.77588094, 0.78141937, 0.79059529, 0.76726752, 0.76155024,\n",
       "        0.77931073, 0.74743589, 0.76087524, 0.76858001, 0.76302716,\n",
       "        0.74858107, 0.74363975, 0.73358403, 0.72945617, 0.75497623,\n",
       "        0.75857333, 0.78627127, 0.74175322, 0.77217999, 0.75913006,\n",
       "        0.75368105, 0.75359739, 0.73049174, 0.75388874, 0.73406576,\n",
       "        0.75678776, 0.74595032, 0.73511576, 0.75900314, 0.75002049,\n",
       "        0.71818892, 0.76103678, 0.74223207, 0.75785795, 0.80321541,\n",
       "        0.77278288, 0.74326475, 0.77055885, 0.77100596, 0.73607921,\n",
       "        0.74220034, 0.72966963, 0.74841088, 0.75620507, 0.75234548,\n",
       "        0.76977424, 0.73324942, 0.7745021 , 0.74210226, 0.78752319,\n",
       "        0.73427057, 0.7699502 , 0.73328115, 0.76893193]),\n",
       " 'split2_test_score': array([0.56191923, 0.66905518, 0.79271984, 0.80467255, 0.77265408,\n",
       "        0.78190012, 0.80690205, 0.7847232 , 0.80196179, 0.78528054,\n",
       "        0.78182102, 0.75244218, 0.76426496, 0.80428677, 0.77567424,\n",
       "        0.77981766, 0.77936106, 0.79141978, 0.80627765, 0.75256358,\n",
       "        0.77063748, 0.78065995, 0.7736947 , 0.77703836, 0.78097759,\n",
       "        0.80531341, 0.80196124, 0.77506166, 0.76879408, 0.77222565,\n",
       "        0.80396914, 0.77134365, 0.80538431, 0.7712926 , 0.75792931,\n",
       "        0.77183144, 0.769072  , 0.77336006, 0.80286026, 0.7762301 ,\n",
       "        0.76724845, 0.81117545, 0.77011566, 0.80987372, 0.77177472,\n",
       "        0.80820898, 0.78276711, 0.81250554, 0.77769065, 0.76128999,\n",
       "        0.78315848, 0.76381687, 0.76716337, 0.78135477, 0.77452849,\n",
       "        0.80551477, 0.76628137, 0.77304809, 0.80751983, 0.76360701,\n",
       "        0.80723055, 0.78123566, 0.78306773, 0.76836584, 0.80088072,\n",
       "        0.77701284, 0.76593537, 0.81804143, 0.80671724, 0.8101715 ,\n",
       "        0.76184017, 0.75764287, 0.77045881, 0.77114229, 0.80619257,\n",
       "        0.80042696, 0.75373201, 0.80200945, 0.74815925, 0.78167524,\n",
       "        0.7809549 , 0.77772468, 0.79952227, 0.77826069, 0.79405728,\n",
       "        0.77592381, 0.77670655, 0.75085913, 0.81269839, 0.74410659,\n",
       "        0.77813874, 0.76743563, 0.76898976, 0.77535661, 0.77288361,\n",
       "        0.77346499, 0.80247172, 0.77876833, 0.7638821 , 0.81552589,\n",
       "        0.80950504, 0.80339626, 0.76740443, 0.7835697 , 0.74687453,\n",
       "        0.80547223, 0.78065428, 0.80545237, 0.76609419, 0.76467051,\n",
       "        0.78418512, 0.76686842, 0.81111873, 0.77559483, 0.77613084,\n",
       "        0.81002119, 0.80052055, 0.79897776, 0.80188183, 0.77424773,\n",
       "        0.78713457, 0.80781477, 0.77539915, 0.79276689, 0.8079509 ,\n",
       "        0.80298504, 0.77138903, 0.77305377, 0.78622421, 0.80641662,\n",
       "        0.77872579, 0.77584157, 0.79864594, 0.80693561, 0.8004298 ,\n",
       "        0.8107699 , 0.74146059, 0.76065472, 0.77766796, 0.80508086,\n",
       "        0.7863575 , 0.78126119, 0.80842451, 0.77248656, 0.775419  ,\n",
       "        0.76346521, 0.79471239, 0.81673403, 0.8004383 , 0.78054651,\n",
       "        0.75396173, 0.77209803, 0.76602896, 0.80175138, 0.77089272,\n",
       "        0.79090079, 0.78842779, 0.7697328 , 0.80990775, 0.78040187,\n",
       "        0.78770461, 0.77931568, 0.81195819, 0.77828905, 0.76125879,\n",
       "        0.77909447, 0.77193921, 0.81091453, 0.75618516, 0.81085214,\n",
       "        0.81561381, 0.77627264, 0.81158383, 0.78881065, 0.81677941,\n",
       "        0.76121909, 0.77793171, 0.76613673, 0.75482388, 0.76724278,\n",
       "        0.80189885, 0.7776396 , 0.81077841, 0.77133798, 0.80006962,\n",
       "        0.77107423, 0.78138881, 0.79761931, 0.81449925, 0.77183995,\n",
       "        0.81501257, 0.77216893, 0.80953056, 0.80307012, 0.77666117,\n",
       "        0.75117109, 0.80050637, 0.76408629, 0.73844307, 0.78516638,\n",
       "        0.75222325, 0.79480031, 0.77965317, 0.77794022, 0.75535705,\n",
       "        0.80848691, 0.7719931 , 0.76970727, 0.80415632, 0.75014446,\n",
       "        0.80625497, 0.77378546, 0.76978952, 0.81250837, 0.77703836,\n",
       "        0.80110477, 0.78673753, 0.77124723, 0.77379397, 0.80446828,\n",
       "        0.76956263, 0.7709778 , 0.7685757 , 0.77744959, 0.79975766,\n",
       "        0.8095334 , 0.77918806, 0.77107707, 0.77744959, 0.74452348,\n",
       "        0.7679376 , 0.81039838, 0.75397307, 0.77424206, 0.80968654,\n",
       "        0.80259367, 0.81058839, 0.78558043, 0.78109103, 0.81308976,\n",
       "        0.78342506, 0.77969004, 0.75838024, 0.79044703, 0.77777289,\n",
       "        0.77779274, 0.78278413, 0.77047016, 0.78243246, 0.76930172,\n",
       "        0.77265956, 0.78520608, 0.77562036, 0.76415436, 0.78450842,\n",
       "        0.80178257, 0.76074831, 0.76833181, 0.75547332, 0.80411094,\n",
       "        0.78435528, 0.78609659, 0.78638019, 0.77384785, 0.77700717,\n",
       "        0.77831173, 0.80629751, 0.77408891, 0.806289  , 0.76228259,\n",
       "        0.81172847, 0.74661646, 0.78075638, 0.77957943, 0.76163031,\n",
       "        0.80715115, 0.77814441, 0.80673142, 0.80929517, 0.76859272,\n",
       "        0.80457888, 0.75498553, 0.81076423, 0.77395562, 0.81071318,\n",
       "        0.80246038, 0.81020269, 0.80380749, 0.7984928 , 0.76548445,\n",
       "        0.80268159, 0.75371783, 0.7908384 , 0.80822316, 0.81445104,\n",
       "        0.77605427, 0.80221365, 0.76071995, 0.77059494, 0.76872317,\n",
       "        0.81435745, 0.76747249, 0.77984318, 0.80311834, 0.80913352,\n",
       "        0.77733615, 0.77536795, 0.74420017, 0.8051631 , 0.8121482 ,\n",
       "        0.80398899, 0.77474119, 0.80363733, 0.80395496, 0.78129522,\n",
       "        0.77631234, 0.77106572, 0.76676916, 0.77796858, 0.78634049,\n",
       "        0.76236767, 0.75953733, 0.80025396, 0.78397809, 0.77983184,\n",
       "        0.76231095, 0.81567053, 0.79250314, 0.79511511, 0.76530011,\n",
       "        0.80028799, 0.77529422, 0.77159606, 0.80363733, 0.77913985,\n",
       "        0.80704054, 0.78502174, 0.7748206 , 0.81681628, 0.80706606,\n",
       "        0.77560334, 0.80868826, 0.75260044, 0.75264866, 0.75239058,\n",
       "        0.80248307, 0.74540264, 0.78491681, 0.80736952, 0.78293443,\n",
       "        0.77409458, 0.76588149, 0.76591836, 0.80472352, 0.77721987,\n",
       "        0.77226819, 0.78934382, 0.77036806, 0.76727964, 0.76977817,\n",
       "        0.80912501, 0.76642884, 0.77927598, 0.74434197, 0.77980915,\n",
       "        0.81341306, 0.78863198, 0.80814375, 0.77747795, 0.76719456,\n",
       "        0.80356359, 0.79683373, 0.75469909, 0.76678901, 0.78256859,\n",
       "        0.79096035, 0.80500712, 0.80333387, 0.74747293, 0.80449097,\n",
       "        0.78563715, 0.78641422, 0.76615091, 0.77747227, 0.78189362,\n",
       "        0.78291175, 0.77763109, 0.78079041, 0.78287204, 0.75225729,\n",
       "        0.75028626, 0.76960801, 0.78014947, 0.7670159 , 0.77751198,\n",
       "        0.8076361 , 0.78997909, 0.76231662, 0.80727877]),\n",
       " 'split3_test_score': array([0.60090308, 0.72748772, 0.75176045, 0.76052084, 0.7741698 ,\n",
       "        0.74463161, 0.77117134, 0.76275816, 0.71252566, 0.72082768,\n",
       "        0.72469504, 0.73971364, 0.71248811, 0.72175357, 0.76647391,\n",
       "        0.6966912 , 0.76924572, 0.73730152, 0.72248069, 0.75019884,\n",
       "        0.75572958, 0.69925416, 0.7748074 , 0.75481939, 0.71078492,\n",
       "        0.72450218, 0.72057265, 0.72902991, 0.70591168, 0.74835784,\n",
       "        0.7601387 , 0.69233623, 0.76252375, 0.7531692 , 0.7731108 ,\n",
       "        0.77473779, 0.72952754, 0.76626763, 0.71425033, 0.78063982,\n",
       "        0.75303254, 0.75896551, 0.71519404, 0.74985333, 0.73179914,\n",
       "        0.75308411, 0.74498783, 0.77454956, 0.75134367, 0.71808962,\n",
       "        0.69773289, 0.77073606, 0.73309094, 0.70586269, 0.73754389,\n",
       "        0.71281156, 0.73245664, 0.75660625, 0.75289072, 0.71092157,\n",
       "        0.7157484 , 0.75174074, 0.72160144, 0.72729978, 0.76721908,\n",
       "        0.75928524, 0.73504539, 0.74494399, 0.74236556, 0.70810592,\n",
       "        0.73365562, 0.72836209, 0.71476602, 0.71889924, 0.7167282 ,\n",
       "        0.72866377, 0.75218681, 0.71606555, 0.72419277, 0.7439874 ,\n",
       "        0.70880984, 0.72003375, 0.75759379, 0.73844892, 0.70729887,\n",
       "        0.74496462, 0.75841631, 0.73245149, 0.75281853, 0.73767539,\n",
       "        0.75729727, 0.74173385, 0.77161015, 0.74467842, 0.75336258,\n",
       "        0.70973549, 0.72326711, 0.72192632, 0.76877903, 0.73932043,\n",
       "        0.71170799, 0.7333591 , 0.71872907, 0.74933764, 0.7342074 ,\n",
       "        0.75235699, 0.72912015, 0.73014121, 0.71990999, 0.72462079,\n",
       "        0.71294306, 0.73583439, 0.75765051, 0.72048756, 0.73184813,\n",
       "        0.7229577 , 0.74955939, 0.7428632 , 0.77257706, 0.71891471,\n",
       "        0.74025898, 0.77614561, 0.72830537, 0.76096122, 0.75930329,\n",
       "        0.72762982, 0.73759804, 0.7210084 , 0.74619969, 0.7443097 ,\n",
       "        0.74867241, 0.72066547, 0.72312272, 0.71760487, 0.75924141,\n",
       "        0.77869826, 0.7300355 , 0.72129719, 0.73587307, 0.75530414,\n",
       "        0.69891381, 0.72406642, 0.75495089, 0.72683824, 0.70420475,\n",
       "        0.69883904, 0.76146401, 0.7434614 , 0.74278585, 0.67512003,\n",
       "        0.75308153, 0.70818843, 0.76268877, 0.71048324, 0.7523441 ,\n",
       "        0.71633628, 0.71682361, 0.76425646, 0.7459831 , 0.76502741,\n",
       "        0.74272912, 0.76013612, 0.71140374, 0.73968399, 0.74180862,\n",
       "        0.69870754, 0.76668792, 0.76800034, 0.75097237, 0.72793407,\n",
       "        0.74228047, 0.71810766, 0.69463619, 0.73923277, 0.72415151,\n",
       "        0.7334777 , 0.71398991, 0.74392551, 0.75410775, 0.74201489,\n",
       "        0.71648325, 0.70627524, 0.77699907, 0.73581892, 0.7148279 ,\n",
       "        0.71763323, 0.7631271 , 0.73392119, 0.72758856, 0.72420566,\n",
       "        0.71921123, 0.74605272, 0.71822112, 0.76715462, 0.72021167,\n",
       "        0.74250995, 0.753623  , 0.75410259, 0.7337768 , 0.71414719,\n",
       "        0.71889924, 0.7353316 , 0.7689492 , 0.72959716, 0.76907813,\n",
       "        0.7367781 , 0.72692075, 0.71427096, 0.72994783, 0.75579146,\n",
       "        0.73177594, 0.69274878, 0.76676785, 0.74427102, 0.74236814,\n",
       "        0.71107112, 0.75374161, 0.77486929, 0.71017383, 0.78588177,\n",
       "        0.73910642, 0.7364197 , 0.72737713, 0.7395396 , 0.69596924,\n",
       "        0.71440504, 0.75041027, 0.7198017 , 0.73104366, 0.69843164,\n",
       "        0.70111321, 0.71430705, 0.74260793, 0.76096896, 0.76699991,\n",
       "        0.75500504, 0.73064143, 0.70386182, 0.74662513, 0.758883  ,\n",
       "        0.77562219, 0.68566324, 0.68812049, 0.73089411, 0.76309358,\n",
       "        0.71402343, 0.73479786, 0.75097495, 0.75094916, 0.72776132,\n",
       "        0.71088032, 0.75472657, 0.75223322, 0.76279706, 0.7330239 ,\n",
       "        0.73947772, 0.75127147, 0.73717775, 0.70054338, 0.73864488,\n",
       "        0.70489062, 0.70821422, 0.75633551, 0.74732905, 0.76361701,\n",
       "        0.74626157, 0.69304272, 0.77426593, 0.72899381, 0.7445134 ,\n",
       "        0.71658123, 0.75034839, 0.71271874, 0.73949577, 0.7423011 ,\n",
       "        0.75846272, 0.73146653, 0.73103593, 0.72671447, 0.71642653,\n",
       "        0.72704193, 0.69677629, 0.7554279 , 0.72908663, 0.73682709,\n",
       "        0.70603286, 0.70722152, 0.7582616 , 0.68700661, 0.71078749,\n",
       "        0.73685545, 0.75646959, 0.72530149, 0.75571153, 0.75297839,\n",
       "        0.75239824, 0.73104624, 0.7620519 , 0.75076094, 0.71219274,\n",
       "        0.74350265, 0.70697399, 0.74448503, 0.74114081, 0.76034755,\n",
       "        0.73814209, 0.74988169, 0.71273937, 0.72231567, 0.7470351 ,\n",
       "        0.72696974, 0.75167886, 0.71157134, 0.76272745, 0.72122499,\n",
       "        0.7551056 , 0.704558  , 0.76417137, 0.73847471, 0.77725434,\n",
       "        0.75224096, 0.73825296, 0.76502999, 0.75794961, 0.70654855,\n",
       "        0.74425813, 0.77072059, 0.76397541, 0.75810174, 0.70305993,\n",
       "        0.7418344 , 0.71321122, 0.72000797, 0.73272996, 0.70540115,\n",
       "        0.74060707, 0.74226242, 0.75209657, 0.75596422, 0.723806  ,\n",
       "        0.75346056, 0.75083829, 0.74374502, 0.77203301, 0.71694221,\n",
       "        0.75543564, 0.72564958, 0.71828816, 0.73502476, 0.71291728,\n",
       "        0.74922935, 0.73328948, 0.72439904, 0.72547683, 0.76869394,\n",
       "        0.74068442, 0.76644813, 0.7807739 , 0.72485542, 0.73198737,\n",
       "        0.71842997, 0.71830878, 0.73727831, 0.7163337 , 0.76087613,\n",
       "        0.76681168, 0.70445744, 0.77043438, 0.69151113, 0.72642053,\n",
       "        0.75166597, 0.77141419, 0.70601224, 0.71520951, 0.72741839,\n",
       "        0.73029334, 0.69894217, 0.71905911, 0.71485626, 0.70715964,\n",
       "        0.76373561, 0.72483995, 0.71879868, 0.73366335, 0.71659155,\n",
       "        0.75103425, 0.74411116, 0.7676471 , 0.7631271 , 0.74069732,\n",
       "        0.73028818, 0.76844641, 0.74591864, 0.76316578, 0.71740117,\n",
       "        0.75155767, 0.74331185, 0.74020226, 0.75788515]),\n",
       " 'split4_test_score': array([0.59301116, 0.7111206 , 0.79377998, 0.86232765, 0.86162707,\n",
       "        0.8663413 , 0.88093492, 0.86945673, 0.85910936, 0.8446338 ,\n",
       "        0.85349474, 0.85735115, 0.85240247, 0.83686508, 0.85250324,\n",
       "        0.82772562, 0.85420552, 0.86125959, 0.86011853, 0.84383681,\n",
       "        0.83246037, 0.84579024, 0.85449388, 0.84801654, 0.85851858,\n",
       "        0.84267095, 0.83005734, 0.83399211, 0.85299005, 0.84914519,\n",
       "        0.84485073, 0.85436985, 0.84242599, 0.84322907, 0.85012501,\n",
       "        0.8456104 , 0.84736849, 0.84477322, 0.8406679 , 0.82786825,\n",
       "        0.85299935, 0.8343921 , 0.84870799, 0.85032035, 0.84421819,\n",
       "        0.8460693 , 0.83339988, 0.83876098, 0.83591144, 0.84317636,\n",
       "        0.83065267, 0.85587059, 0.84747392, 0.8536846 , 0.83731605,\n",
       "        0.84768786, 0.84889713, 0.85540859, 0.85696203, 0.84853745,\n",
       "        0.84224615, 0.86284714, 0.84663363, 0.84663673, 0.85353267,\n",
       "        0.85668917, 0.83545874, 0.8541249 , 0.85616825, 0.85024283,\n",
       "        0.84126013, 0.84781189, 0.83677653, 0.84344302, 0.84928472,\n",
       "        0.85019942, 0.85571245, 0.85842556, 0.84991106, 0.83223402,\n",
       "        0.8492072 , 0.85746124, 0.82532568, 0.8527854 , 0.83830517,\n",
       "        0.84320427, 0.84438563, 0.8357099 , 0.8590767 , 0.8447143 ,\n",
       "        0.83464946, 0.85519154, 0.84055628, 0.84346782, 0.84131905,\n",
       "        0.83891291, 0.84457167, 0.83740907, 0.8352882 , 0.84302133,\n",
       "        0.85405978, 0.83046663, 0.83652228, 0.84475461, 0.83893772,\n",
       "        0.84323527, 0.84932813, 0.86033248, 0.83113018, 0.85697443,\n",
       "        0.83606027, 0.85219007, 0.8527606 , 0.83947413, 0.83284486,\n",
       "        0.85297764, 0.82754268, 0.84871109, 0.83672692, 0.84869869,\n",
       "        0.84188337, 0.85512332, 0.8577217 , 0.84818398, 0.85827672,\n",
       "        0.832572  , 0.83721993, 0.84372518, 0.84672665, 0.85002269,\n",
       "        0.84384611, 0.8330526 , 0.84517941, 0.83632073, 0.85866121,\n",
       "        0.84586466, 0.82326682, 0.85877594, 0.8438244 , 0.8518986 ,\n",
       "        0.84728788, 0.84095937, 0.83923538, 0.83923848, 0.85422102,\n",
       "        0.83819665, 0.8397718 , 0.84280738, 0.85711086, 0.85621476,\n",
       "        0.84344302, 0.84401355, 0.82575358, 0.84972812, 0.84159501,\n",
       "        0.84438563, 0.84300582, 0.84359495, 0.83870206, 0.83235495,\n",
       "        0.85337763, 0.8312263 , 0.84588326, 0.84075162, 0.84745841,\n",
       "        0.84662743, 0.83472078, 0.84805375, 0.84455617, 0.84053457,\n",
       "        0.83182783, 0.83866796, 0.84216864, 0.85693102, 0.84991416,\n",
       "        0.85185519, 0.82681091, 0.82971316, 0.85915732, 0.82552103,\n",
       "        0.84015319, 0.83471147, 0.8563543 , 0.82280792, 0.84549878,\n",
       "        0.83913306, 0.83790829, 0.83902764, 0.84130974, 0.83981831,\n",
       "        0.84894675, 0.86412153, 0.82905271, 0.84567242, 0.84262754,\n",
       "        0.83829897, 0.85045988, 0.85739613, 0.85554812, 0.84149268,\n",
       "        0.84055938, 0.84217484, 0.83609438, 0.84714834, 0.83855323,\n",
       "        0.8446957 , 0.84453446, 0.85089708, 0.83639515, 0.8527668 ,\n",
       "        0.85017462, 0.84435772, 0.84551738, 0.83959816, 0.85460241,\n",
       "        0.83559827, 0.83278594, 0.84234227, 0.85223348, 0.83921058,\n",
       "        0.82531638, 0.84253452, 0.8348386 , 0.82843878, 0.85811549,\n",
       "        0.83997955, 0.84892194, 0.8357285 , 0.84871109, 0.82267149,\n",
       "        0.84223995, 0.86143633, 0.84161051, 0.85597911, 0.8397811 ,\n",
       "        0.85730311, 0.82993951, 0.8514428 , 0.84356705, 0.832603  ,\n",
       "        0.84749872, 0.85089088, 0.84188957, 0.84676386, 0.84914829,\n",
       "        0.84213763, 0.83477039, 0.84703982, 0.83285416, 0.83238906,\n",
       "        0.84810956, 0.85394506, 0.85183349, 0.84653131, 0.83541843,\n",
       "        0.84898085, 0.83345569, 0.85401327, 0.8568194 , 0.84753593,\n",
       "        0.84941185, 0.83749899, 0.85086917, 0.83729745, 0.85712017,\n",
       "        0.8424787 , 0.83782767, 0.84457477, 0.84406316, 0.8402338 ,\n",
       "        0.83610058, 0.8379827 , 0.8545776 , 0.84910488, 0.8397563 ,\n",
       "        0.84266165, 0.82196763, 0.84256862, 0.85756357, 0.85731241,\n",
       "        0.83999815, 0.84522592, 0.8420198 , 0.85536828, 0.85621166,\n",
       "        0.86330915, 0.84899946, 0.85025214, 0.84961029, 0.84271746,\n",
       "        0.84992966, 0.83754861, 0.83945553, 0.85400397, 0.85017152,\n",
       "        0.82743105, 0.85286292, 0.85405978, 0.84831731, 0.84781499,\n",
       "        0.84510499, 0.83017516, 0.83471457, 0.84743671, 0.85774341,\n",
       "        0.85219627, 0.8343859 , 0.83051314, 0.83099685, 0.85314198,\n",
       "        0.84316086, 0.83766333, 0.85087538, 0.84843513, 0.8550179 ,\n",
       "        0.84576234, 0.84135005, 0.84149579, 0.84230507, 0.86909193,\n",
       "        0.83477039, 0.83292238, 0.85917903, 0.84826149, 0.8298744 ,\n",
       "        0.85779612, 0.85124126, 0.85519154, 0.8500692 , 0.83635484,\n",
       "        0.83764163, 0.86149214, 0.83504015, 0.84163532, 0.82520165,\n",
       "        0.84609101, 0.85376212, 0.84946146, 0.83621841, 0.83819355,\n",
       "        0.82964494, 0.83370375, 0.83850982, 0.85184899, 0.83614709,\n",
       "        0.84948316, 0.84503987, 0.8429283 , 0.86737725, 0.83550215,\n",
       "        0.83405103, 0.84003226, 0.84285389, 0.84676076, 0.83687886,\n",
       "        0.85300555, 0.83459055, 0.83274874, 0.86207507, 0.85500239,\n",
       "        0.83531611, 0.83313632, 0.84571272, 0.85610004, 0.83270533,\n",
       "        0.85738683, 0.84976843, 0.82801708, 0.84255932, 0.84181516,\n",
       "        0.83836409, 0.82671479, 0.82815971, 0.85827672, 0.83949894,\n",
       "        0.85169086, 0.84616853, 0.84934363, 0.84509259, 0.85346445,\n",
       "        0.85597911, 0.83800131, 0.83633624, 0.83925709, 0.83690366,\n",
       "        0.84989556, 0.83264951, 0.85885345, 0.84193918, 0.84938704,\n",
       "        0.82548382, 0.83677653, 0.85766589, 0.8424601 , 0.83957026,\n",
       "        0.84284459, 0.84871109, 0.84148338, 0.86655867]),\n",
       " 'mean_test_score': array([0.52845437, 0.66637734, 0.76719872, 0.79168755, 0.76917966,\n",
       "        0.76537596, 0.77607284, 0.76419919, 0.7642148 , 0.76581981,\n",
       "        0.76554797, 0.76288628, 0.76484005, 0.76681041, 0.77848204,\n",
       "        0.7434604 , 0.77877981, 0.7747786 , 0.76533947, 0.76109761,\n",
       "        0.76269918, 0.76446652, 0.7669719 , 0.75933579, 0.75659494,\n",
       "        0.77018861, 0.75819916, 0.76428688, 0.76198967, 0.77058715,\n",
       "        0.77061365, 0.74378897, 0.78589522, 0.7617715 , 0.76751515,\n",
       "        0.76880156, 0.75511731, 0.76736342, 0.77005333, 0.77345478,\n",
       "        0.76256927, 0.76891199, 0.76198635, 0.76795381, 0.75971018,\n",
       "        0.76244671, 0.76567617, 0.77987725, 0.76243846, 0.75605616,\n",
       "        0.765013  , 0.78503744, 0.7511023 , 0.75412292, 0.77605741,\n",
       "        0.77238015, 0.7636252 , 0.77184387, 0.76790334, 0.75129339,\n",
       "        0.76842125, 0.77702404, 0.7574116 , 0.76177568, 0.77227509,\n",
       "        0.77146179, 0.75922723, 0.76587284, 0.76376306, 0.75992438,\n",
       "        0.76890161, 0.75905592, 0.74745272, 0.76341869, 0.77026452,\n",
       "        0.76359935, 0.76516245, 0.77081054, 0.76358733, 0.75957235,\n",
       "        0.75991257, 0.76115618, 0.76124601, 0.76669296, 0.75572996,\n",
       "        0.75883628, 0.77060443, 0.74569008, 0.76425429, 0.74813575,\n",
       "        0.75191247, 0.75931839, 0.76771408, 0.76883136, 0.76262061,\n",
       "        0.75893407, 0.75633462, 0.76127369, 0.76123558, 0.77076891,\n",
       "        0.76820914, 0.76458014, 0.75166273, 0.77726591, 0.76455859,\n",
       "        0.77031744, 0.77489619, 0.7765338 , 0.74834558, 0.76131   ,\n",
       "        0.7631914 , 0.76194216, 0.76026979, 0.76111957, 0.76185715,\n",
       "        0.77789404, 0.77157081, 0.77464605, 0.77439415, 0.76218766,\n",
       "        0.78009335, 0.78904176, 0.77016433, 0.77928918, 0.76930338,\n",
       "        0.77342192, 0.77242257, 0.75741259, 0.77535674, 0.77952174,\n",
       "        0.7754902 , 0.7621781 , 0.77163683, 0.75553065, 0.77167948,\n",
       "        0.76678434, 0.75303174, 0.7614209 , 0.76540686, 0.77088035,\n",
       "        0.76133299, 0.76831118, 0.78333052, 0.75112422, 0.75884526,\n",
       "        0.75088911, 0.78103453, 0.7715991 , 0.76869403, 0.75537154,\n",
       "        0.76102352, 0.76869476, 0.77531127, 0.7728076 , 0.75878362,\n",
       "        0.77544764, 0.76904866, 0.76866223, 0.76888324, 0.76845853,\n",
       "        0.77474861, 0.7648545 , 0.75848247, 0.76948806, 0.7578117 ,\n",
       "        0.75565317, 0.75738603, 0.77795559, 0.75879857, 0.76492982,\n",
       "        0.76267565, 0.75986804, 0.76310609, 0.76498742, 0.77780218,\n",
       "        0.76977996, 0.76622013, 0.75011138, 0.76236208, 0.75205511,\n",
       "        0.76388138, 0.7588434 , 0.78351075, 0.76462129, 0.75981171,\n",
       "        0.74770939, 0.77157907, 0.77088872, 0.76973723, 0.76016632,\n",
       "        0.77537171, 0.77299671, 0.75582228, 0.77972739, 0.76410377,\n",
       "        0.76261706, 0.77220107, 0.76844606, 0.75653163, 0.76676706,\n",
       "        0.7568504 , 0.75815476, 0.75922674, 0.76571302, 0.76743167,\n",
       "        0.76116675, 0.77241879, 0.76183528, 0.77239986, 0.75821792,\n",
       "        0.77029363, 0.75064642, 0.76695988, 0.7719915 , 0.75578736,\n",
       "        0.75397507, 0.77095397, 0.7725358 , 0.76086577, 0.76293262,\n",
       "        0.74048238, 0.76186246, 0.74173197, 0.75908109, 0.77415409,\n",
       "        0.76638083, 0.76341059, 0.75450978, 0.76356558, 0.73914655,\n",
       "        0.73850834, 0.76934839, 0.75839614, 0.76256488, 0.77292875,\n",
       "        0.77293198, 0.76644266, 0.77081094, 0.76907943, 0.77277296,\n",
       "        0.78114016, 0.76345413, 0.74717676, 0.75079392, 0.76667378,\n",
       "        0.74977328, 0.77116011, 0.76290756, 0.77151083, 0.76293567,\n",
       "        0.74722501, 0.7773379 , 0.7657725 , 0.74995803, 0.75512558,\n",
       "        0.76945895, 0.7594376 , 0.76813661, 0.76285766, 0.764538  ,\n",
       "        0.76743723, 0.76239709, 0.78009648, 0.76364723, 0.76862431,\n",
       "        0.76896866, 0.75426799, 0.76697429, 0.77092384, 0.74942561,\n",
       "        0.76347575, 0.75863225, 0.75705241, 0.77622639, 0.75058335,\n",
       "        0.78009069, 0.7542886 , 0.76502069, 0.77215668, 0.7475092 ,\n",
       "        0.76920488, 0.75043077, 0.77167922, 0.77609818, 0.76957819,\n",
       "        0.7600591 , 0.77880416, 0.77535523, 0.75055323, 0.74783887,\n",
       "        0.7740452 , 0.76865479, 0.76721374, 0.78248073, 0.77152627,\n",
       "        0.75805174, 0.77614248, 0.76261369, 0.77431503, 0.75282541,\n",
       "        0.77436386, 0.75716104, 0.7662793 , 0.76199844, 0.77986321,\n",
       "        0.76844772, 0.77360892, 0.75046144, 0.75677151, 0.76902085,\n",
       "        0.77466297, 0.775039  , 0.75476029, 0.76974534, 0.76792444,\n",
       "        0.77000588, 0.75450854, 0.76997216, 0.75379206, 0.78066519,\n",
       "        0.75156656, 0.7506326 , 0.78490815, 0.76217776, 0.74983562,\n",
       "        0.77491517, 0.7819306 , 0.76946552, 0.76562583, 0.73995271,\n",
       "        0.76520702, 0.77479004, 0.76030975, 0.77130481, 0.75130865,\n",
       "        0.76160461, 0.76955344, 0.77149163, 0.77863324, 0.7720081 ,\n",
       "        0.76155968, 0.77134348, 0.75946076, 0.76942722, 0.74668808,\n",
       "        0.77681707, 0.76492721, 0.77266169, 0.77630588, 0.75929662,\n",
       "        0.75618239, 0.76109376, 0.76784921, 0.76634453, 0.76470623,\n",
       "        0.76878709, 0.77580663, 0.77340047, 0.7619419 , 0.75404655,\n",
       "        0.75322447, 0.75345986, 0.75410707, 0.75919615, 0.76837994,\n",
       "        0.78611451, 0.75789248, 0.77488972, 0.75189557, 0.75564004,\n",
       "        0.77389111, 0.76417495, 0.75555894, 0.76697145, 0.76301787,\n",
       "        0.76112488, 0.76496385, 0.76484171, 0.75987361, 0.77381757,\n",
       "        0.76596326, 0.76006794, 0.76444639, 0.77198746, 0.76495051,\n",
       "        0.76223479, 0.76509284, 0.77768012, 0.77949727, 0.75661903,\n",
       "        0.75472973, 0.77077714, 0.77206067, 0.76387769, 0.76127895,\n",
       "        0.76644827, 0.77002807, 0.74627921, 0.78717387]),\n",
       " 'std_test_score': array([0.07223137, 0.06815283, 0.05093893, 0.05387732, 0.06301219,\n",
       "        0.06122629, 0.07462657, 0.07237694, 0.06196999, 0.0455248 ,\n",
       "        0.04832906, 0.04888746, 0.04956415, 0.04679022, 0.0467392 ,\n",
       "        0.05733593, 0.04397913, 0.05492417, 0.05891414, 0.04445633,\n",
       "        0.04893655, 0.04832298, 0.06292326, 0.05726666, 0.06513172,\n",
       "        0.04592685, 0.05279751, 0.04908833, 0.05040012, 0.04231591,\n",
       "        0.05970397, 0.0679859 , 0.03321085, 0.04844703, 0.04720719,\n",
       "        0.04918285, 0.05304245, 0.04345566, 0.04542424, 0.04513009,\n",
       "        0.05781415, 0.04922754, 0.05045862, 0.05695601, 0.05455854,\n",
       "        0.063942  , 0.03838274, 0.04589082, 0.04463633, 0.04706546,\n",
       "        0.044645  , 0.03799118, 0.05731616, 0.05845454, 0.03633162,\n",
       "        0.04841072, 0.04968324, 0.04390894, 0.05983496, 0.05254931,\n",
       "        0.04838949, 0.04540584, 0.05692498, 0.04552366, 0.05390358,\n",
       "        0.04590255, 0.04898478, 0.06497283, 0.06728211, 0.06163516,\n",
       "        0.04556597, 0.04610787, 0.06016923, 0.05296463, 0.05109704,\n",
       "        0.0529048 , 0.04887642, 0.0523481 , 0.05253239, 0.05114773,\n",
       "        0.05145833, 0.05661071, 0.04747076, 0.04637419, 0.05165583,\n",
       "        0.04919147, 0.04050973, 0.05615344, 0.06976212, 0.05214295,\n",
       "        0.05682505, 0.05379597, 0.0456969 , 0.050909  , 0.04398148,\n",
       "        0.0514778 , 0.0614778 , 0.04677179, 0.06103724, 0.05078234,\n",
       "        0.05444909, 0.0457977 , 0.05586439, 0.03628226, 0.03789574,\n",
       "        0.04941373, 0.0421292 , 0.0508196 , 0.04906951, 0.05007698,\n",
       "        0.04297045, 0.0532586 , 0.06870497, 0.04465457, 0.04385674,\n",
       "        0.04704348, 0.0407081 , 0.05163296, 0.04665038, 0.04697429,\n",
       "        0.03944175, 0.03902291, 0.04658022, 0.03888311, 0.06859902,\n",
       "        0.04004333, 0.03415424, 0.0484908 , 0.03903634, 0.04248345,\n",
       "        0.03964953, 0.04034486, 0.04444142, 0.05663684, 0.05880915,\n",
       "        0.0675621 , 0.03667745, 0.05038364, 0.04969825, 0.05497789,\n",
       "        0.05552747, 0.04064164, 0.03528235, 0.05088501, 0.05292589,\n",
       "        0.0549637 , 0.03555326, 0.05075958, 0.05287497, 0.06202106,\n",
       "        0.04697619, 0.04875446, 0.02867276, 0.0491587 , 0.0480994 ,\n",
       "        0.05001512, 0.04343318, 0.04601074, 0.05158514, 0.04043099,\n",
       "        0.05059147, 0.040001  , 0.0586137 , 0.04038914, 0.05180012,\n",
       "        0.05797146, 0.05145294, 0.05027532, 0.04824625, 0.05261363,\n",
       "        0.05164051, 0.04995027, 0.05529485, 0.05912074, 0.04924574,\n",
       "        0.04347492, 0.03711483, 0.04867477, 0.05213445, 0.05479284,\n",
       "        0.05331242, 0.04489087, 0.05233163, 0.0377373 , 0.05822326,\n",
       "        0.05620745, 0.03818545, 0.04173986, 0.05253319, 0.04608967,\n",
       "        0.05084734, 0.04941903, 0.05468707, 0.04009467, 0.04365058,\n",
       "        0.03804896, 0.04885353, 0.05348012, 0.05506501, 0.04392523,\n",
       "        0.04317311, 0.05442351, 0.052268  , 0.04452721, 0.03777857,\n",
       "        0.06163669, 0.043763  , 0.0493876 , 0.04082753, 0.05241579,\n",
       "        0.05248642, 0.05494703, 0.04434415, 0.045276  , 0.05815875,\n",
       "        0.06255533, 0.03526456, 0.03948396, 0.05004388, 0.06665684,\n",
       "        0.05925902, 0.04578014, 0.06029887, 0.05752475, 0.05392098,\n",
       "        0.05334401, 0.0503425 , 0.05790326, 0.04725466, 0.05041766,\n",
       "        0.0673244 , 0.05980719, 0.04390422, 0.05666576, 0.04834698,\n",
       "        0.05187636, 0.04909456, 0.05020379, 0.04061926, 0.043435  ,\n",
       "        0.03635064, 0.05498322, 0.05358296, 0.06407529, 0.04722058,\n",
       "        0.06046355, 0.03679362, 0.05109052, 0.03431346, 0.04157239,\n",
       "        0.06027186, 0.04157545, 0.05089066, 0.0655168 , 0.05522058,\n",
       "        0.05016648, 0.0458864 , 0.04548743, 0.0515354 , 0.05336173,\n",
       "        0.04969505, 0.04529932, 0.03992688, 0.04018712, 0.04933344,\n",
       "        0.04524801, 0.05926503, 0.05076785, 0.04616248, 0.06092695,\n",
       "        0.0514457 , 0.04672405, 0.05523777, 0.04124569, 0.05945225,\n",
       "        0.04024321, 0.04707854, 0.05008889, 0.05247461, 0.0623471 ,\n",
       "        0.04680495, 0.05369774, 0.05262673, 0.0427434 , 0.05803595,\n",
       "        0.06500712, 0.05034759, 0.0531901 , 0.06275674, 0.05929352,\n",
       "        0.04759902, 0.04182099, 0.04565647, 0.0454743 , 0.06842508,\n",
       "        0.0418397 , 0.0453345 , 0.05633076, 0.03783287, 0.05704814,\n",
       "        0.04713827, 0.04182036, 0.03826809, 0.05581814, 0.04720902,\n",
       "        0.04468398, 0.0323962 , 0.04145292, 0.05668643, 0.06207666,\n",
       "        0.04570194, 0.03422169, 0.06396076, 0.06023649, 0.04815615,\n",
       "        0.04709304, 0.05258659, 0.04013492, 0.05297913, 0.04934962,\n",
       "        0.05738166, 0.04591785, 0.04519097, 0.05473272, 0.04876241,\n",
       "        0.0459305 , 0.04973385, 0.05765972, 0.05934919, 0.0610501 ,\n",
       "        0.04887541, 0.05286374, 0.0409425 , 0.04847378, 0.04695943,\n",
       "        0.06206863, 0.04625956, 0.04171018, 0.03994824, 0.04332218,\n",
       "        0.04755494, 0.04764492, 0.04602809, 0.04367557, 0.05952356,\n",
       "        0.04934644, 0.04481743, 0.04580221, 0.05692493, 0.04822158,\n",
       "        0.06245288, 0.04242659, 0.04019699, 0.05664095, 0.05089659,\n",
       "        0.04439493, 0.03379193, 0.03388522, 0.05248731, 0.06333777,\n",
       "        0.0676543 , 0.05828181, 0.05853759, 0.05355111, 0.03972746,\n",
       "        0.04345793, 0.05739331, 0.03828246, 0.05634148, 0.04707836,\n",
       "        0.04202048, 0.04990839, 0.04036701, 0.04912224, 0.04453743,\n",
       "        0.05303047, 0.05514264, 0.05331196, 0.04497012, 0.05988287,\n",
       "        0.06572928, 0.04694891, 0.04111485, 0.03779897, 0.0418869 ,\n",
       "        0.05400668, 0.03735603, 0.04377089, 0.03287838, 0.05205657,\n",
       "        0.04263511, 0.03576211, 0.05095514, 0.04509612, 0.05462464,\n",
       "        0.05246419, 0.04992755, 0.06003597, 0.04603583]),\n",
       " 'rank_test_score': array([399, 398, 168,   1, 134, 193,  41, 218, 217, 186, 191, 239, 207,\n",
       "        173,  27, 392,  25,  55, 194, 279, 241, 213, 170, 297, 328, 116,\n",
       "        315, 215, 257, 112, 110, 391,   5, 265, 163, 143, 342, 166, 118,\n",
       "         67, 246, 139, 258, 158, 293, 248, 189,  18, 249, 332, 199,   6,\n",
       "        366, 349,  42,  80, 225,  88, 160, 364, 153,  34, 321, 264,  81,\n",
       "         98, 300, 185, 223, 288, 140, 304, 385, 231, 115, 226, 196, 107,\n",
       "        227, 294, 289, 276, 273, 176, 335, 308, 111, 390, 216, 381, 359,\n",
       "        298, 162, 142, 243, 305, 330, 272, 274, 109, 156, 210, 361,  33,\n",
       "        211, 113,  52,  36, 380, 270, 233, 259, 284, 278, 262,  29,  94,\n",
       "         58,  59, 253,  16,   2, 117,  23, 132,  68,  77, 320,  47,  21,\n",
       "         44, 254,  91, 339,  89, 174, 356, 268, 192, 105, 269, 155,   9,\n",
       "        365, 306, 367,  13,  92, 146, 340, 281, 145,  49,  73, 310,  45,\n",
       "        136, 147, 141, 150,  56, 205, 312, 127, 319, 336, 322,  28, 309,\n",
       "        203, 242, 291, 234, 200,  30, 122, 183, 375, 251, 358, 221, 307,\n",
       "          8, 209, 292, 383,  93, 104, 124, 285,  46,  70, 333,  20, 220,\n",
       "        244,  82, 152, 329, 175, 325, 316, 301, 188, 165, 275,  78, 263,\n",
       "         79, 314, 114, 369, 172,  86, 334, 352, 102,  76, 282, 237, 394,\n",
       "        261, 393, 303,  62, 180, 232, 345, 228, 396, 397, 131, 313, 247,\n",
       "         72,  71, 179, 106, 135,  74,  12, 230, 387, 368, 177, 378, 101,\n",
       "        238,  96, 236, 386,  32, 187, 376, 341, 129, 296, 157, 240, 212,\n",
       "        164, 250,  15, 224, 149, 138, 348, 169, 103, 379, 229, 311, 324,\n",
       "         38, 371,  17, 347, 198,  83, 384, 133, 374,  90,  40, 125, 287,\n",
       "         24,  48, 372, 382,  63, 148, 167,  10,  95, 317,  39, 245,  61,\n",
       "        357,  60, 323, 182, 256,  19, 151,  66, 373, 326, 137,  57,  50,\n",
       "        343, 123, 159, 120, 346, 121, 353,  14, 362, 370,   7, 255, 377,\n",
       "         51,  11, 128, 190, 395, 195,  54, 283, 100, 363, 266, 126,  97,\n",
       "         26,  85, 267,  99, 295, 130, 388,  35, 204,  75,  37, 299, 331,\n",
       "        280, 161, 181, 208, 144,  43,  69, 260, 351, 355, 354, 350, 302,\n",
       "        154,   4, 318,  53, 360, 337,  64, 219, 338, 171, 235, 277, 201,\n",
       "        206, 290,  65, 184, 286, 214,  87, 202, 252, 197,  31,  22, 327,\n",
       "        344, 108,  84, 222, 271, 178, 119, 389,   3], dtype=int32)}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-10T15:50:27.875008Z",
     "start_time": "2021-05-10T15:50:27.834261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 4}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
